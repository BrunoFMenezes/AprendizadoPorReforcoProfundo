{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ENIAC"
      ],
      "metadata": {
        "id": "HcSSFsVtC1tX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu-WIYcX9mHM"
      },
      "source": [
        "# Base para Todas Versões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXpB_90z9Wi7"
      },
      "source": [
        "## Instalações"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensordict\n",
        "!pip install torchrl"
      ],
      "metadata": {
        "id": "-Um7q47b7Mb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensordict-nightly\n",
        "# !pip install torchrl-nightly"
      ],
      "metadata": {
        "id": "qu3I-JcHNV_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118"
      ],
      "metadata": {
        "id": "CJNZUAKHPKQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xSzHJZ_-LtU"
      },
      "outputs": [],
      "source": [
        "# !pip install tensordict # Esse pacote é uma biblioteca que fornece um dicionário de tensores\n",
        "# tensordict-0.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJVXgyxi9dhT"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade torchrl\n",
        "!pip install torchrl # torchrl é um complemento focado em fornecer ferramentas e utilitários específicos para Aprendizado por Reforço."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIuYwlD1-KFT"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch torchvision\n",
        "# torch refere-se ao PyTorch, biblioteca de aprendizado de máquina utilizada para aplicações de deep learning.\n",
        "# torchvision é uma biblioteca que complementa o PyTorch com ferramentas específicas para visão computacional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edOy2cE_-WJ2"
      },
      "outputs": [],
      "source": [
        "!pip install av # av é uma biblioteca de Python que proporciona uma interface simplificada para a manipulação de áudio e vídeo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BJ6-pb8-hoi"
      },
      "source": [
        "## Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLe_Lmh0-kRr"
      },
      "outputs": [],
      "source": [
        "import torch #  permitindo operações avançadas de aprendizado de máquina e manipulação de tensores.\n",
        "import time #  oferece funcionalidades relacionadas ao tempo, como obter a hora atual, pausar a execução do programa, e medir intervalos de tempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "127_gjFD-uA7"
      },
      "outputs": [],
      "source": [
        "import torchvision  # fornece:\n",
        "# Modelos de redes neurais pré-treinados.\n",
        "# Conjuntos de dados populares para visão computacional.\n",
        "# Ferramentas para transformar e manipular imagens.\n",
        "# Utilitários adicionais para facilitar o trabalho com dados de imagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc3B7J6z-xJE"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q230vEyp_AJX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "DnsaOtab8n9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensordict import TensorDict, tensorclass"
      ],
      "metadata": {
        "id": "MRNyE47s-k_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDQQTo_s-21j"
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import GymEnv, StepCounter, TransformedEnv\n",
        "# GymEnv: Classe que integra ambientes do OpenAI Gym com a biblioteca torchrl, permitindo o uso de ambientes padrão de Reinforcement Learning.\n",
        "# StepCounter: Um wrapper para contar o número de passos realizados em um ambiente.\n",
        "# TransformedEnv: Um wrapper que aplica transformações ao ambiente, como normalização de observações ou recompensas.\n",
        "from torchrl.modules import EGreedyModule, MLP, QValueModule\n",
        "# EGreedyModule: Implementa uma política epsilon-greedy, que escolhe ações aleatoriamente com probabilidade epsilon, e a melhor ação com probabilidade 1-epsilon.\n",
        "# MLP: Multi-Layer Perceptron, uma rede neural feed-forward composta por múltiplas camadas densamente conectadas.\n",
        "# QValueModule: Um módulo específico para calcular valores Q, usados em algoritmos de Q-learning.\n",
        "from torchrl.collectors import SyncDataCollector # Coletor de dados síncrono, usado para coletar experiências do ambiente de forma síncrona\n",
        "from torchrl.data import LazyMemmapStorage, LazyTensorStorage, ListStorage, ReplayBuffer, PrioritizedReplayBuffer, TensorDictReplayBuffer\n",
        "# LazyMemmapStorage: Armazena tensores de maneira eficiente em um arquivo de mapa de memória.\n",
        "# LazyTensorStorage: Armazena tensores de maneira eficiente, com carregamento atrasado.\n",
        "# ReplayBuffer: Buffer de replay, usado para armazenar e amostrar experiências anteriores, crucial para métodos de RL como DQN.\n",
        "from torchrl.objectives import DQNLoss, SoftUpdate\n",
        "# DQNLoss: Implementa a função de perda para o algoritmo DQN (Deep Q-Network).\n",
        "# SoftUpdate: Função para atualização suave dos parâmetros do modelo, frequentemente usada em algoritmos de RL para atualizar gradualmente uma rede alvo.\n",
        "from torchrl._utils import logger as torchrl_logger # Utilitário de registro usado para registrar mensagens de depuração e informações durante a execução.\n",
        "from torchrl.record import CSVLogger, VideoRecorder\n",
        "# CSVLogger: Logger que registra dados em um arquivo CSV, útil para monitorar o treinamento e resultados.\n",
        "# VideoRecorder: Grava vídeos das execuções dos ambientes, útil para visualizar o comportamento de agentes de RL.\n",
        "\n",
        "from tensordict.nn import TensorDictModule as Mod, TensorDictSequential as Seq\n",
        "# TensorDictModule (Mod): Um módulo que opera sobre TensorDicts, que são dicionários especializados em armazenar tensores.\n",
        "# TensorDictSequential (Seq): Similar ao torch.nn.Sequential, mas opera sobre TensorDicts,\n",
        "# permitindo a construção de modelos sequenciais que trabalham com múltiplos tensores de entrada e saída.\n",
        "\n",
        "from torch.optim import Adam # Adam: Um otimizador amplamente utilizado, que combina as vantagens dos métodos AdaGrad e RMSProp,\n",
        "# ajustando os passos de atualização com base em estimativas de momentos de primeira e segunda ordem dos gradientes.\n",
        "\n",
        "from torchrl.data.replay_buffers.samplers import PrioritizedSampler, SamplerWithoutReplacement, RandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF9PutVV_CK-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')   # Monta o Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbQOfvlOd1yI"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYC1V47CFWr8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque"
      ],
      "metadata": {
        "id": "t419KMnp2XDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torchrl.data"
      ],
      "metadata": {
        "id": "YNL0Q08gpF0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils._pytree import tree_map"
      ],
      "metadata": {
        "id": "cQGM2Y4LAR4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UmYgAQSkLdv"
      },
      "source": [
        "## Parâmetros Gerais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVtrGNtYkPTF"
      },
      "outputs": [],
      "source": [
        "nome_jogo = \"CartPole-v1\"     # Define o nome do ambiente de RL a ser utilizado.\n",
        "num_celulas = [128, 128]        # Define a arquitetura da rede neural, com duas camadas ocultas de 64 neurônios cada.\n",
        "passos_decaimento = 345       # Número de passos até o valor mínimo de epsilon\n",
        "epsilon_ini = 1               # Valor inicial de epsilon\n",
        "epsilon_fim = 0.001           # Valor mínimo de epsilon\n",
        "passos_aleatorios_ini = 500   # Define o número inicial de passos aleatórios a serem realizados no ambiente antes de começar a usar a política treinada.\n",
        "exp_coletadas = 500           # Define o número de frames (ou passos de tempo) que serão coletados em cada lote (batch) de coleta de dados do ambiente.\n",
        "passos_otimizacao = 10        # Define o número de passos de otimização que serão realizados após cada lote de coleta de dados.\n",
        "tamanho_buffer = 100000       # Define o tamanho do buffer de replay.\n",
        "alfa = 0.6                    # Parâmetro de correção de viés de importância. Valores maiores de beta aumentam a correção.\n",
        "beta = 1.0                    # O parâmetro de correção de viés de importância. Valores maiores de beta aumentam a correção.\n",
        "funcao_perda = \"l2\"           # Pode ser “l1”, “l2” ou “smooth_l1”.\n",
        "valor_delay = True            # O padrão é False para não criar uma rede alvo.\n",
        "taxa_aprendizado = 0.001      # Define a taxa de aprendizado (learning rate) para o otimizador.\n",
        "tau = 0.99                    # Define o fator de atualização suave da rede alvo para a política de exploração.\n",
        "renderizado_pixel = True      # Indica que o ambiente será renderizado a partir de pixels, permitindo a gravação visual do ambiente.\n",
        "somente_pixel = False         # Tanto as observações em forma de pixels quanto outras observações (como a posição e a velocidade do carrinho) serão retornadas pelo ambiente.\n",
        "caminho = \"./training_loop\"   # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "formato_video = \"mp4\"         # Especifica o formato de vídeo para gravação.\n",
        "episodios = 10000              # Define o número total de episódios a serem treinados.\n",
        "quantidade_paradas = 5        # Número de paradas em 500 pontos\n",
        "tamanho_amostra = 500         # Tamanho dO MINIBATCH\n",
        "passos_maximo = 500           # Número máximo de passos para cada episódio.\n",
        "recompensa_maxima = 500       # Define a recompensa máxima que o agente pode receber."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyNmPdbNSTUW"
      },
      "outputs": [],
      "source": [
        "nome_pasta_1 = \"_\" + nome_jogo + \"_RN-\" + str(num_celulas[0])+\"-\"+str(num_celulas[1])+\"_EG-\"+str(passos_decaimento)+\"-\"+str(epsilon_ini)+\"-\"+str(epsilon_fim)\n",
        "nome_pasta_2 = '_CD-' + str(passos_aleatorios_ini) + '-' + str(exp_coletadas) + '-' + str(passos_otimizacao)\n",
        "nome_pasta_3 = '_RB-' + str(tamanho_buffer/1000) + 'k-' + str(alfa) + '-' + str(beta)\n",
        "nome_pasta_4 = '_O-' + str(funcao_perda) + '-' + str(taxa_aprendizado) + '_AR-' + str(tau)\n",
        "nome_pasta_5 = '_T-' + str(episodios/1000) + 'k-' + str(quantidade_paradas) + '-' + str(tamanho_amostra)\n",
        "nome_pasta = nome_pasta_1 + nome_pasta_2 + nome_pasta_3 + nome_pasta_4 + nome_pasta_5\n",
        "print(nome_pasta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4F4pUkqFNBr"
      },
      "source": [
        "## Construindo o ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiiMyLr-FNBr"
      },
      "source": [
        "Estaremos usando um ambiente de academia com uma StepCounter transformação. Se precisar de uma atualização, verifique se esses recursos são apresentados no tutorial do ambiente ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7YzMz6TFNBr"
      },
      "outputs": [],
      "source": [
        "def criar_ambiente(nome_jogo, semente = 0):\n",
        "    torch.manual_seed(semente) # define a semente manual para a geração de números aleatórios no PyTorch, garantindo a reprodutibilidade dos resultados.\n",
        "    env = TransformedEnv(GymEnv(nome_jogo), StepCounter()) # cria o ambiente \"CartPole-v1\" do OpenAI Gym, com um contador de passos adicionado\n",
        "    env.set_seed(semente) # define a semente para o ambiente env, garantindo a consistência do comportamento do ambiente em diferentes execuções.\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wH2IHjfFNBs"
      },
      "source": [
        "## Projetando uma política"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR2BWMrlFNBt"
      },
      "source": [
        "O próximo passo é construir nossa política. Faremos uma versão regular e determinística do ator para ser usada no módulo de perda e durante a avaliação . A seguir, iremos aumentá-lo com um módulo de exploração para inferência ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkjElFfR6M8J"
      },
      "source": [
        "### Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJZlKlTOFNBt"
      },
      "outputs": [],
      "source": [
        "def criar_rede_neural(ambiente, num_celulas):\n",
        "    # Cria uma MLP que mapeia observações do ambiente para valores de ação, com duas camadas ocultas de 64 neurônios cada.\n",
        "    value_mlp = MLP(out_features = ambiente.action_spec.shape[-1], num_cells = num_celulas)\n",
        "\n",
        "    # Cria um módulo que pega as observações do ambiente (observation) e produz valores de ação (action_value) usando a MLP definida anteriormente.\n",
        "    value_net = Mod(value_mlp, in_keys = [\"observation\"], out_keys = [\"action_value\"])\n",
        "    return value_net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-rheTgo6Pl4"
      },
      "source": [
        "### Política e Política de Exploração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0Jlun-dFNBu"
      },
      "outputs": [],
      "source": [
        "def criar_politica(ambiente, rede_valor, passos_decaimento, epsilon_ini, epsilon_fim):\n",
        "    # Cria política que primeiro passa a observação através da value_net para obter os valores de ação\n",
        "    # e, em seguida, usa o QValueModule para calcular os valores Q para as ações.\n",
        "    policy = Seq(rede_valor, QValueModule(spec = ambiente.action_spec))\n",
        "\n",
        "    # Cria um módulo de exploração que implementa uma estratégia epsilon-greedy com um valor inicial de epsilon de 0.5 e que diminui ao longo de 100.000 passos.\n",
        "    exploration_module = EGreedyModule(ambiente.action_spec, annealing_num_steps = passos_decaimento, eps_init = epsilon_ini, eps_end = epsilon_fim)\n",
        "\n",
        "    # Combina a política com o módulo de exploração em uma sequência, resultando em uma política que primeiro calcula os valores de ação\n",
        "    # e, em seguida, aplica a estratégia epsilon-greedy para selecionar as ações.\n",
        "    policy_explore = Seq(policy, exploration_module)  # Seq: Refere-se ao TensorDictSequential.\n",
        "\n",
        "    return policy, exploration_module, policy_explore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhxLwjMAu_Op"
      },
      "source": [
        "Passando de e- decay = 0,98 para propocional em passos:\n",
        "\n",
        "\\begin{align}\n",
        " ϵ_{\\mbox{inicial}} * ϵ_{\\mbox{decay}}^{n_p} = ϵ_{\\mbox{mínimo}}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        " n_p = (\\frac{\\log ϵ_{\\mbox{mínimo}} - \\log ϵ_{\\mbox{inicial}} }{\\log ϵ_{\\mbox{decay}}})\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        " n_p ≈ 345\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coletor de Dados"
      ],
      "metadata": {
        "id": "wW4J0O7Nudi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_coletor(ambiente, politica, exp_coletadas, passos_aleatorios_ini, exp_totais = -1):\n",
        "    # Um coletor de dados síncrono que coleta experiências do ambiente usando a política fornecida.\n",
        "    collector = SyncDataCollector(\n",
        "    ambiente,                                     # O ambiente de RL, criado anteriormente com TransformedEnv.\n",
        "    politica,                                     # A política usada para coletar experiências do ambiente.\n",
        "    frames_per_batch = exp_coletadas,             # Especifica que 200 frames serão coletados em cada lote.\n",
        "    total_frames = exp_totais,                    # Especifica que o coletor deve continuar indefinidamente (ou até que seja explicitamente parado).\n",
        "    init_random_frames = passos_aleatorios_ini,   # init_random_frames=init_rand_steps: 5000 passos aleatórios realizados inicialmente p/ preencher o rb.\n",
        "    )\n",
        "    return collector"
      ],
      "metadata": {
        "id": "5afNvgHTuhRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buffer de Replay"
      ],
      "metadata": {
        "id": "NvA3FYLaOWnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_Buffer_Replay(tamanho_buffer, tipo_armazenamento = LazyTensorStorage):\n",
        "    rb = ReplayBuffer(storage = tipo_armazenamento(tamanho_buffer)) # Buffer de replay usado p/ armazenar experiências anteriores p/ amostragem durante o treinamento.\n",
        "    return rb"
      ],
      "metadata": {
        "id": "Q_zmfxg2OaGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_Buffer_Replay_Amostra_PER(tamanho_buffer, alfa, b, tipo_armazenamento = LazyTensorStorage):\n",
        "    rb = ReplayBuffer(\n",
        "    storage = tipo_armazenamento(tamanho_buffer),\n",
        "    sampler = PrioritizedSampler(\n",
        "        max_capacity = tamanho_buffer,            # A capacidade máxima do amostrador, que deve ser consistente com o tamanho do buffer de replay.\n",
        "        alpha = alfa,                             # Controla quão fortemente a prioridade afeta a amostragem. Valores maiores de @: amostragem dependa mais das prioridades.\n",
        "        beta = b)                                 # O parâmetro de correção de viés de importância. Valores maiores de beta aumentam a correção.\n",
        "    )\n",
        "    return rb"
      ],
      "metadata": {
        "id": "asTNJt9wlodP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_Buffer_Replay_PER(tamanho_buffer, alfa, b, tipo_armazenamento = LazyTensorStorage):\n",
        "    rb = PrioritizedReplayBuffer(alpha = alfa, beta = b, storage = tipo_armazenamento(tamanho_buffer))\n",
        "    return rb"
      ],
      "metadata": {
        "id": "j4XwEFBPxZEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Módulo de Perda"
      ],
      "metadata": {
        "id": "Gebr-b5IPPWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_perda(politica, ambiente, funcao_perda, reducao, valor_delay = True):\n",
        "    #  Implementa a função de perda para o algoritmo DQN (Deep Q-Network).\n",
        "    loss = DQNLoss(value_network = politica, action_space = ambiente.action_spec, loss_function = funcao_perda, delay_value = valor_delay, reduction = reducao)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "l9h16QrSPSJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Otimizador"
      ],
      "metadata": {
        "id": "9kK9SfbpPTkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_otimizador(perca, taxa_aprendizado):\n",
        "    # Um otimizador amplamente utilizado que combina as vantagens dos métodos AdaGrad e RMSProp.\n",
        "    optim = Adam(perca.parameters(), lr = taxa_aprendizado)\n",
        "    return optim"
      ],
      "metadata": {
        "id": "T_Ec6G6gPWLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Atualização da Rede Alvo"
      ],
      "metadata": {
        "id": "O8EGGc2ZPcBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_atualizador_rede_alvo(perca, tau):\n",
        "    # Implementa a atualização suave dos parâmetros da rede, uma técnica comum em algoritmos de RL para atualizar gradualmente uma rede alvo (target network) com os parâmetros da rede de valor (online network).\n",
        "    updater = SoftUpdate(perca, eps = tau)\n",
        "    return updater"
      ],
      "metadata": {
        "id": "OsVvZFcKPgn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Registrador"
      ],
      "metadata": {
        "id": "ZRrANVhGvyIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_registrador(nome_experiencia, caminho, formato_video, etiqueta, nome_jogo, renderizado_pixel = True, somente_pixel = False):\n",
        "    # CSVLogger: Um logger que registra dados em formato CSV, útil para monitorar o progresso do treinamento e analisar resultados.\n",
        "    logger = CSVLogger(exp_name = nome_experiencia, log_dir = caminho, video_format = formato_video)\n",
        "\n",
        "    # Um objeto que grava vídeos das execuções do ambiente durante o treinamento, útil para visualizar o comportamento do agente.\n",
        "    video_recorder = VideoRecorder(logger, tag = etiqueta)\n",
        "\n",
        "    # Um wrapper que aplica transformações ao ambiente, como normalização de observações ou recompensas, e também permite a adição de funcionalidades como gravação de vídeos.\n",
        "    record_env = TransformedEnv(GymEnv(nome_jogo, from_pixels = renderizado_pixel, pixels_only = somente_pixel), video_recorder)\n",
        "    return logger, video_recorder, record_env"
      ],
      "metadata": {
        "id": "VY4T660ev0G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeGeXQHsdIry"
      },
      "source": [
        "## Funções Auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w5Q7GUm7SaQ"
      },
      "source": [
        "### Plotagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTaYlTqqdMmF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Plotar Simples\n",
        "def plotar(dados,\n",
        "           xlabel = 'Episódio',\n",
        "           ylabel = 'Recompensa',\n",
        "           limit = True,\n",
        "           xlimit = 1000,\n",
        "           ylimit = 1000,\n",
        "           title = 'Desempenho do Agente'):\n",
        "\n",
        "    plt.plot(dados)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "\n",
        "    if limit:\n",
        "      xlimit = [0,(xlimit*1.05)]\n",
        "      ylimit = [0,(ylimit*1.05)]\n",
        "      plt.xlim(xlimit)\n",
        "      plt.ylim(ylimit)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotar Versões\n",
        "def plotar_versoes(\n",
        "                  #  dados,\n",
        "                   coluna = 'Rewards_ep',\n",
        "                   xlabel = 'Episódio',\n",
        "                   ylabel = 'Recompensa',\n",
        "                   tamanho_fig = (10, 6),\n",
        "                   grade = False,\n",
        "                   limit = True,\n",
        "                   xlimit = 1000,\n",
        "                   ylimit = 1000,\n",
        "                   title = 'Desempenho do Agente'):\n",
        "    plt.figure(figsize=tamanho_fig)\n",
        "\n",
        "    plt.plot(resultado_medio_V1[coluna],          label = \"V1 (DQN)\",                                         linestyle = 'solid')\n",
        "    plt.plot(resultado_medio_V2_rb[coluna] ,      label = \"V2 (ReplayBuffer)\",                                linestyle = 'dotted')\n",
        "    # plt.plot(resultado_medio_V2_prb[coluna] ,     label = \"V2 (PrioritazedReplayBuffer)\",                     linestyle = 'dotted')\n",
        "    plt.plot(resultado_medio_V2_1[coluna] ,       label = \"V2.1 (PER Média Amostral)\",                        linestyle = 'dotted')\n",
        "    plt.plot(resultado_medio_V2_2[coluna] ,       label = \"V2.2 (PER Soma Amostral)\",                         linestyle = 'dotted')\n",
        "    plt.plot(resultado_medio_V3_1v[coluna] ,      label = \"V3 (PER Último Índice)\",    linestyle = 'dashed')\n",
        "    # plt.plot(resultado_medio_V3_2v[coluna] ,      label = \"V3 (PER Últimos Índices - Priorização 2ª Vez)\",    linestyle = 'dashed')\n",
        "    # plt.plot(resultado_medio_V3_1[coluna] ,       label = \"V3.1 (PER Melhores Índices - Max)\",                linestyle = 'dashed')\n",
        "    # plt.plot(resultado_medio_V3_2_sm1[coluna] ,   label = \"V3.2 (PER Melhores Índices - Semi-Trajetória 1)\",  linestyle = 'dashed')\n",
        "    # plt.plot(resultado_medio_V3_2_sm2[coluna] ,   label = \"V3.2 (PER Melhores Índices - Semi-Trajetória 2)\",  linestyle = 'dashed')\n",
        "    # plt.plot(resultado_medio_V3_3[coluna] ,       label = \"V3.3 (PER Piores Índices - Max)\",                  linestyle = 'dashed')\n",
        "    # plt.plot(resultado_medio_V3_4_sm1[coluna] ,   label = \"V3.4 (PER Piores Índices - Semi-Trajetória 1)\",    linestyle = 'dashed')\n",
        "    # plt.plot(resultado_medio_V3_4_sm2[coluna] ,   label = \"V3.4 (PER Piores Índices - Semi-Trajetória 2)\",    linestyle = 'dashed')\n",
        "    plt.plot(resultado_medio_V4[coluna] ,         label = \"V4 (PER Última Trajetória)\",                       linestyle = 'dashdot')\n",
        "    # plt.plot(resultado_medio_V4_1[coluna] ,       label = \"V4.1 (PER Melhores Trajetórias)\",                  linestyle = 'dashdot')\n",
        "    # plt.plot(resultado_medio_V4_2[coluna] ,       label = \"V4.2 (PER Fortalecimento da Base)\",                linestyle = 'dashdot')\n",
        "    # plt.plot(resultado_medio_V4_3[coluna] ,       label = \"V4.3 (PER Piores Trajetórias)\",                    linestyle = 'dashdot')\n",
        "\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    # plt.legend()\n",
        "    # plt.legend(bbox_to_anchor=(0.08, -0.18, 0.9, .102), loc = 'center', ncols = 3, mode = \"expand\", borderaxespad = 0.)\n",
        "    plt.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.1), fancybox = True, shadow = True, ncol = 3)\n",
        "    plt.grid(grade)\n",
        "\n",
        "    # plt.plot(dados)\n",
        "\n",
        "    if limit:\n",
        "        xlimit = [0, (xlimit * 1.05)]\n",
        "        ylimit = [0, (ylimit * 1.05)]\n",
        "        plt.xlim(xlimit)\n",
        "        plt.ylim(ylimit)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4DRH3x8uNFTN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuL6rQ7mFOPq"
      },
      "source": [
        "### Aviso de Conclusão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoEBw55vFSWb"
      },
      "outputs": [],
      "source": [
        "def play_sound(tempo, nota = 0):\n",
        "    # Cria um som (um simples tom sinusoidal)\n",
        "    framerate = 44100               # Taxa de amostragem\n",
        "    duration = tempo + 1            # Duração em segundos\n",
        "\n",
        "    if nota == 1:\n",
        "        frequency = 261.625519            # Frequência do som em Hz (C3)\n",
        "    elif nota == 2:\n",
        "        frequency = 277.182648            # Frequência do som em Hz (C#3)\n",
        "    elif nota == 3:\n",
        "        frequency = 293.664734            # Frequência do som em Hz (D3)\n",
        "    elif nota == 4:\n",
        "        frequency = 311.126984            # Frequência do som em Hz (D#3)\n",
        "    elif nota == 5:\n",
        "        frequency = 329.627533            # Frequência do som em Hz (E3)\n",
        "    elif nota == 6:\n",
        "        frequency = 349.228241            # Frequência do som em Hz (F3)\n",
        "    elif nota == 7:\n",
        "        frequency = 369.994385            # Frequência do som em Hz (F#3)\n",
        "    elif nota == 8:\n",
        "        frequency = 391.995392            # Frequência do som em Hz (G3)\n",
        "    elif nota == 9:\n",
        "        frequency = 415.304688            # Frequência do som em Hz (G#3)\n",
        "    elif nota == 10:\n",
        "        frequency = 440.0                 # Frequência do som em Hz (A3)\n",
        "    elif nota == 11:\n",
        "        frequency = 466.163761            # Frequência do som em Hz (A#3)\n",
        "    else:\n",
        "        frequency = 493.883301            # Frequência do som em Hz (B3)\n",
        "\n",
        "    t = np.linspace(0, duration, int(framerate * duration))\n",
        "    data = np.sin(2 * np.pi * frequency * t)\n",
        "\n",
        "    # Toca o som\n",
        "    display(Audio(data, rate=framerate, autoplay=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bEMoH1JFi7E"
      },
      "outputs": [],
      "source": [
        "# # Exemplo de simulação (substitua esta parte pelo seu código de simulação)\n",
        "# print(\"Iniciando simulação...\")\n",
        "# time.sleep(5)  # Simula um processo que leva tempo para terminar\n",
        "# print(\"Simulação concluída!\")\n",
        "\n",
        "# # Toca o som quando a simulação termina\n",
        "# play_sound(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuHn9QXf7OyQ"
      },
      "source": [
        "### Salvar e Carregar Dados em Excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsYY8IYv7OFB"
      },
      "outputs": [],
      "source": [
        "#@title Salvando Resultados\n",
        "def salvar_array(score_list_1,\n",
        "                 score_list_mean_100,\n",
        "                 eval_score,\n",
        "                 mean_100_eval_score,\n",
        "                 eval_score_mean,\n",
        "                 Sucessos,\n",
        "                 Desempenho_Medio,\n",
        "                 tempo_treinamento,\n",
        "                 tempo_avaliacao,\n",
        "                 tempo_simulacao,\n",
        "                 cenario=\"C1\",\n",
        "                 versao=\"V1\",\n",
        "                 numero_simulacao = 1,\n",
        "                 caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                 pasta = \"teste\"):\n",
        "\n",
        "    recomp_ep = np.array(score_list_1)#.reshape(-1, 1)\n",
        "    recomp_med_ep = np.array(score_list_mean_100)#.reshape(-1, 1)\n",
        "    recomp_ep_eval = np.array(eval_score)#.reshape(-1, 1)\n",
        "    recomp_med_ep_eval = np.array(mean_100_eval_score)#.reshape(-1, 1)\n",
        "    recomp_med_eval_ep = np.array(eval_score_mean).reshape(1, )\n",
        "    sucess_rates = np.array(Sucessos)#.reshape(-1, 1)\n",
        "    taxa_med_eval_ep = np.array(Desempenho_Medio).reshape(1, )\n",
        "    tempo_treinamento = np.array(tempo_treinamento).reshape(1, )\n",
        "    tempo_avaliacao = np.array(tempo_avaliacao).reshape(1, )\n",
        "    tempo_simulacao = np.array(tempo_simulacao).reshape(1, )\n",
        "\n",
        "\n",
        "    array_coluna1 = recomp_ep\n",
        "    array_coluna2 = recomp_med_ep\n",
        "    array_coluna3 = recomp_ep_eval\n",
        "    array_coluna4 = recomp_med_ep_eval\n",
        "    array_coluna5 = sucess_rates\n",
        "    array_coluna6 = recomp_med_eval_ep\n",
        "    array_coluna7 = taxa_med_eval_ep\n",
        "    array_coluna8 = tempo_treinamento\n",
        "    array_coluna9 = tempo_avaliacao\n",
        "    array_coluna10 = tempo_simulacao\n",
        "\n",
        "\n",
        "    # Preenchendo os arrays com tamanhos diferentes para que tenham o mesmo tamanho\n",
        "    tamanho_maximo = max(len(array_coluna1),\n",
        "                         len(array_coluna2),\n",
        "                         len(array_coluna3),\n",
        "                         len(array_coluna4),\n",
        "                         len(array_coluna5),\n",
        "                         len(array_coluna6),\n",
        "                         len(array_coluna7),\n",
        "                         len(array_coluna8),\n",
        "                         len(array_coluna9),\n",
        "                         len(array_coluna10)\n",
        "                         )\n",
        "    array_coluna1 = np.pad(array_coluna1, (0, tamanho_maximo - len(array_coluna1)), mode='constant', constant_values=-9999)\n",
        "    array_coluna2 = np.pad(array_coluna2, (0, tamanho_maximo - len(array_coluna2)), mode='constant', constant_values=-9999)\n",
        "    array_coluna3 = np.pad(array_coluna3, (0, tamanho_maximo - len(array_coluna3)), mode='constant', constant_values=-9999)\n",
        "    array_coluna4 = np.pad(array_coluna4, (0, tamanho_maximo - len(array_coluna4)), mode='constant', constant_values=-9999)\n",
        "    array_coluna5 = np.pad(array_coluna5, (0, tamanho_maximo - len(array_coluna5)), mode='constant', constant_values=-9999)\n",
        "    array_coluna6 = np.pad(array_coluna6, (0, tamanho_maximo - len(array_coluna6)), mode='constant', constant_values=-9999)\n",
        "    array_coluna7 = np.pad(array_coluna7, (0, tamanho_maximo - len(array_coluna7)), mode='constant', constant_values=-9999)\n",
        "    array_coluna8 = np.pad(array_coluna8, (0, tamanho_maximo - len(array_coluna8)), mode='constant', constant_values=-9999)\n",
        "    array_coluna9 = np.pad(array_coluna9, (0, tamanho_maximo - len(array_coluna9)), mode='constant', constant_values=-9999)\n",
        "    array_coluna10 = np.pad(array_coluna10, (0, tamanho_maximo - len(array_coluna10)), mode='constant', constant_values=-9999)\n",
        "\n",
        "    # Criando o DataFrame com os arrays de colunas\n",
        "    df = pd.DataFrame({'Rewards_ep': array_coluna1,\n",
        "                        'Rewards_mean_ep': array_coluna2,\n",
        "                        'Rewards_ep_eval': array_coluna3,\n",
        "                        'Rewards_mean_ep_eval': array_coluna4,\n",
        "                        'Sucess_Rates': array_coluna5,\n",
        "                        'Mean_Rewards_ep_eval': array_coluna6,\n",
        "                        'Mean_Sucess_Rates': array_coluna7,\n",
        "                        'Tempo_treinamento': array_coluna8,\n",
        "                        'Tempo_avaliacao': array_coluna9,\n",
        "                        'Tempo_simulacao': array_coluna10}\n",
        "                      )\n",
        "\n",
        "    # Substituindo os valores de preenchimento por None\n",
        "    df = df.replace(-9999, None)\n",
        "\n",
        "    # Salvando o DataFrame em uma planilha Excel'\n",
        "    nome_da_pasta = str(cenario) + pasta\n",
        "    caminho_pasta =  caminho + nome_da_pasta  # Define o caminho da pasta no Google Drive\n",
        "    if not os.path.exists(caminho_pasta): os.makedirs(caminho_pasta) # Cria a pasta se ela não existir\n",
        "    nome_arquivo = '/Resultado_de_'+ str(versao)+'_simu_'+ str(numero_simulacao)  +'.xlsx'\n",
        "    caminho_completo =  caminho_pasta + nome_arquivo\n",
        "    # caminho_completo = os.path.join(caminho_pasta, nome_arquivo)\n",
        "\n",
        "    df.to_excel(caminho_completo, index=False)\n",
        "\n",
        "    print(\"DataFrame salvo em\", caminho_completo)\n",
        "    play_sound(5, numero_simulacao)\n",
        "    return caminho_completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ7xJWYE7dDQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Carregando Resultados\n",
        "def carrega_array(cenario,\n",
        "                  versao,\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = \"teste\"):\n",
        "    # Substitua o caminho do arquivo pelo caminho real no seu Google Drive\n",
        "    # caminho_do_arquivo = '/content/drive/My Drive/CBA 2024/Resultados//Result_of_Epoch_'+ str(epoch) + '_' + str(versao) +'.xlsx'\n",
        "    # nome_arquivo = '/Result_of_Epoch_'+ str(epoch) + '_' + str(versao) +'.xlsx'\n",
        "    caminho_pasta =  caminho + str(cenario) + pasta + '/'\n",
        "    nome_arquivo = '/Resultado_de_'+ str(versao)+'_simu_'+ str(numero_simulacao)  +'.xlsx'\n",
        "    caminho_completo = caminho_pasta + nome_arquivo\n",
        "\n",
        "    # Carregue a planilha usando o pandas\n",
        "    # df = pd.read_excel(caminho_do_arquivo)\n",
        "    df = pd.ExcelFile(caminho_completo)\n",
        "    # acoes = df.parse('Ações (A)')\n",
        "    # resultado = df.parse()\n",
        "    resultado = df.parse('Sheet1')\n",
        "\n",
        "    return resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qRq8k_sBFyE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Carregando Várias Simulações\n",
        "def carrega_varios(cenario, versao, inicio = 1, fim = 10, caminho = '/content/drive/My Drive/CBA 2024/Resultados/', pasta = \"teste\"):\n",
        "    resultado = []\n",
        "\n",
        "    for i in range(inicio, (fim+1)):\n",
        "      resultado.append(carrega_array(cenario = str(cenario), versao = str(versao), numero_simulacao = i, caminho = caminho, pasta = pasta ))\n",
        "\n",
        "    return resultado"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Carregando Resultados Médios\n",
        "def carrega_media_versoes(cenario,\n",
        "                          versao = [\"V1\", \"V2_rb\", \"V2_prb\", \"V2.1\", \"V2.2\"],\n",
        "                          # versao,\n",
        "                          inicio = 1,\n",
        "                          fim = 10,\n",
        "                          caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                          pasta = \"teste\"):\n",
        "    resultado_versoes = []\n",
        "    # resultado_medio = None\n",
        "\n",
        "    for i in versao:\n",
        "        resultado = carrega_varios(cenario = str(cenario), versao = str(i), inicio = inicio, fim = fim, caminho = caminho, pasta = pasta)\n",
        "        resultado_med = resultado_medio(resultado)\n",
        "        resultado_versoes.append(resultado_med)\n",
        "\n",
        "    resultado_medio_V1 = resultado_versoes[0]\n",
        "    resultado_medio_V2_rb = resultado_versoes[1]\n",
        "    # resultado_medio_V2_prb = resultado_versoes[2]\n",
        "    resultado_medio_V2_1 = resultado_versoes[2]\n",
        "    resultado_medio_V2_2 = resultado_versoes[3]\n",
        "    resultado_medio_V3_1v = resultado_versoes[4]\n",
        "    # resultado_medio_V3_2v = resultado_versoes[6]\n",
        "    # resultado_medio_V3_1 = resultado_versoes[5]\n",
        "    # resultado_medio_V3_2_sm1 = resultado_versoes[6]\n",
        "    # resultado_medio_V3_2_sm2 = resultado_versoes[9]\n",
        "    # resultado_medio_V3_3 = resultado_versoes[5]\n",
        "    # resultado_medio_V3_4_sm1 = resultado_versoes[11]\n",
        "    # resultado_medio_V3_4_sm2 = resultado_versoes[8]\n",
        "    resultado_medio_V4 = resultado_versoes[5]\n",
        "    # resultado_medio_V4_1 = resultado_versoes[10]\n",
        "    # resultado_medio_V4_2 = resultado_versoes[15]\n",
        "    # resultado_medio_V4_3 = resultado_versoes[11]\n",
        "\n",
        "\n",
        "    return resultado_medio_V1, resultado_medio_V2_rb, resultado_medio_V2_1, resultado_medio_V2_2, resultado_medio_V3_1v, resultado_medio_V4"
      ],
      "metadata": {
        "id": "ljjbFDpYYHCO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLLhpYzv7ga1"
      },
      "source": [
        "### Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPh49LC17lDx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Avaliar Agente\n",
        "def avaliar(env, policy, max_steps=500):\n",
        "    tempo_ini_avaliacao = time.time()\n",
        "    eval_score = []\n",
        "    mean_100_eval_score = []\n",
        "\n",
        "    for i in range(1001):\n",
        "        eval_score.append(env.rollout(max_steps, policy)[\"next\",\"step_count\"][-1].item())\n",
        "        if (i%100)==0:\n",
        "          if i!=0:\n",
        "            mean_100_eval_score.append(np.mean(eval_score[-100]))\n",
        "\n",
        "    eval_score_mean = np.mean(mean_100_eval_score)\n",
        "    # print(f\"Mean 100 eval score: {eval_score_mean}\")\n",
        "\n",
        "    # plt.plot(mean_100_eval_score)\n",
        "    # plt.show()\n",
        "\n",
        "    return eval_score, mean_100_eval_score, eval_score_mean, tempo_ini_avaliacao"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cILb_AhF96oR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Calcular Taxa de Sucesso\n",
        "def taxa_sucesso(mean_100_eval_score):\n",
        "    Sucessos = [0]\n",
        "\n",
        "    for i in mean_100_eval_score:\n",
        "      bb = (i/500)*100\n",
        "      Sucessos.append(bb)\n",
        "\n",
        "    Desempenho_Medio = np.mean(Sucessos[-10])\n",
        "    print(f\"Desempenho Medio: {Desempenho_Medio}\")\n",
        "\n",
        "    plt.plot(Sucessos)\n",
        "    plt.ylim(0,100)\n",
        "    plt.show()\n",
        "\n",
        "    return Sucessos, Desempenho_Medio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Xj4AbDCTok"
      },
      "source": [
        "### Cálculo de Resultados Médios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8-V87a5CXiY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Resultado Médio\n",
        "def resultado_medio(resultado):\n",
        "    soma_resultado = 0\n",
        "\n",
        "    for i in resultado:\n",
        "        soma_resultado += i\n",
        "\n",
        "    resultado_medio = soma_resultado/(len(resultado))\n",
        "\n",
        "    return resultado_medio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Misc36E5FNBW"
      },
      "source": [
        "# V1: DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descrição"
      ],
      "metadata": {
        "id": "aNcegwE_1UyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Deep Q-Network (DQN) é um algoritmo de Aprendizado por Reforço (Reinforcement Learning) desenvolvido pelo Google DeepMind. Ele combina Q-Learning, um método de aprendizado por reforço clássico, com redes neurais profundas, para lidar com ambientes de alta dimensionalidade, como jogos de vídeo.\n",
        "\n",
        "### Componentes principais do DQN:\n",
        "\n",
        "1. **Q-Learning**: O DQN utiliza Q-Learning para aprender uma função de valor Q, que estima a recompensa esperada de se tomar uma ação em um estado específico e seguir a política ótima daí em diante.\n",
        "\n",
        "2. **Redes Neurais Profundas**: Em vez de usar uma tabela Q tradicional, o DQN usa uma rede neural profunda para aproximar a função Q. A entrada da rede é o estado do ambiente, e a saída é o valor Q para todas as ações possíveis nesse estado.\n",
        "\n",
        "3. **Replay Experience**: Para evitar correlações entre sequências de experiências, o DQN armazena as transições (estado, ação, recompensa, próximo estado) em uma memória de replay. Amostras aleatórias dessa memória são usadas para treinar a rede neural, o que melhora a eficiência do aprendizado.\n",
        "\n",
        "4. **Rede de Alvo (Target Network)**: Para estabilizar o treinamento, o DQN utiliza uma segunda rede neural, chamada rede de alvo, que é uma cópia periódica da rede Q principal. A rede de alvo fornece os valores Q durante a atualização do aprendizado, reduzindo as oscilações e a divergência no treinamento.\n",
        "\n",
        "### Funcionamento Básico:\n",
        "\n",
        "1. **Inicialização**: Inicializa-se a rede Q com pesos aleatórios e uma rede de alvo como uma cópia da rede Q.\n",
        "2. **Iteração**: Para cada passo do episódio, realiza-se a seguinte sequência:\n",
        "   - Seleciona-se uma ação usando uma política ε-greedy.\n",
        "   - Executa-se a ação e observa-se a recompensa e o próximo estado.\n",
        "   - Armazena-se a transição na memória de replay.\n",
        "   - Amostra-se um mini-batch aleatório da memória de replay e usa-se para treinar a rede Q.\n",
        "   - Atualiza-se a rede de alvo periodicamente para ser uma cópia da rede Q.\n",
        "\n",
        "### Vantagens:\n",
        "\n",
        "- **Eficiência em Ambientes Complexos**: O uso de redes neurais permite que o DQN lide com estados de alta dimensionalidade.\n",
        "- **Generalização**: A capacidade de generalização das redes neurais permite que o DQN aprenda políticas eficazes em diferentes estados, mesmo aqueles não vistos durante o treinamento.\n",
        "\n",
        "### Aplicações:\n",
        "\n",
        "- **Jogos de Vídeo**: O DQN foi famoso por seu sucesso em jogos do Atari, onde superou o desempenho humano em vários títulos.\n",
        "- **Robótica e Automação**: Pode ser aplicado para ensinar robôs a realizar tarefas complexas.\n",
        "- **Problemas de Decisão e Controle**: Em áreas como finanças, saúde e logística.\n",
        "\n",
        "O DQN representa um passo significativo na combinação de técnicas de aprendizado profundo com aprendizado por reforço, mostrando como essas duas áreas podem se complementar para resolver problemas complexos e de alta dimensionalidade."
      ],
      "metadata": {
        "id": "M-1Ky_6V1XQc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1cmgjkulNNu"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11Yb4CsnlQtR"
      },
      "outputs": [],
      "source": [
        "reducao = \"mean\"              # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V1\"   # Define o nome da experiência.\n",
        "etiqueta = \"video_V1\"         # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Agente V1"
      ],
      "metadata": {
        "id": "zl-xqbYxX5-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V1:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.rb = criar_Buffer_Replay(self.tamanho_buffer)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao             # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "\n",
        "            for passo_atual in dado[\"next\",\"step_count\"]:\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "            self.rb.extend(dado)                                     # Grava dados no buffer de repetição\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay, acessando o campo \"step_count\".\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra = self.rb.sample(self.tamanho_amostra)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                              # Atualiza o contador total de passos coletados.\n",
        "            self.total_episodios += dado[\"next\", \"done\"].sum()             # Atualiza o contador total de episódios completados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if self.total_episodios >= self.episodios:\n",
        "                break\n",
        "\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "6dT8Ja-f1qOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "hFTanQvMUZ_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V1 = Mod_Agente_V1(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                              tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V1.treinar_agente(episodios, tamanho_amostra)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V1.ambiente, agente_V1.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V1\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "35tY_rvJNrr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk2F3wTRczfC"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkmvsw9Mcp1v"
      },
      "outputs": [],
      "source": [
        "resultado_1_V1 = carrega_array(\"C1\",\n",
        "                  \"V1\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38_Ub0p1B8No"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V1\",\n",
        "                           inicio = 1,\n",
        "                           fim = 20,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISr05RrbDgZT"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V1 = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw-UMtzoc_Zw"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V1['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sPMOjYRcZ-h"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V1['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfFT6wD7dC3R"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V1['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4euQaoTjfBSN"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V1['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNFIj_5kdFx_"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V1['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estatísticas"
      ],
      "metadata": {
        "id": "h3rUG92Nib5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media_recompensa_treinamento_V1 = np.mean(resultado_medio_V1['Rewards_ep'])\n",
        "print(f\"A recompensa média do treinamento é: {media_recompensa_treinamento_V1}\")\n",
        "\n",
        "media_recompensa_avaliacao_V1 = np.mean(resultado_medio_V1['Rewards_ep_eval'])\n",
        "print(f\"A recompensa média da avaliação é: {media_recompensa_avaliacao_V1}\")\n",
        "\n",
        "media_taxa_sucesso_V1 = np.mean(resultado_medio_V1['Sucess_Rates'])\n",
        "print(f\"A taxa de sucesso média é: {media_taxa_sucesso_V1}\")"
      ],
      "metadata": {
        "id": "wTVKIYeqifG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XGks7Fg-V3r"
      },
      "source": [
        "# V2: PER (ReplayBuffer+Amostra Priorizada)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descrição"
      ],
      "metadata": {
        "id": "E4voKNlf31S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Prioritized Experience Replay (PER) é uma melhoria do método tradicional de Experience Replay utilizado em algoritmos de Aprendizado por Reforço, como o DQN. O objetivo do PER é aumentar a eficiência do treinamento, dando prioridade às experiências mais importantes durante o aprendizado.\n",
        "\n",
        "### Componentes principais do PER:\n",
        "\n",
        "1. **Experience Replay**: Armazena transições (estado, ação, recompensa, próximo estado) em uma memória de replay. Durante o treinamento, amostras aleatórias dessa memória são usadas para atualizar a rede neural.\n",
        "\n",
        "2. **Prioritização das Experiências**: Em vez de amostrar transições aleatoriamente, o PER atribui uma prioridade a cada transição, baseada na magnitude do seu erro temporal (TD error). Transições com maiores erros são mais prováveis de serem amostradas, pois são consideradas mais informativas para o aprendizado.\n",
        "\n",
        "3. **Erro Temporal (TD Error)**: O TD error é a diferença entre o valor Q esperado e o valor Q atual. Ele indica o quanto a rede está errando ao estimar o valor da ação em um determinado estado. Transições com maior TD error têm maior prioridade.\n",
        "\n",
        "### Funcionamento Básico:\n",
        "\n",
        "1. **Inicialização**: Inicializa-se a memória de replay com uma estrutura que permite armazenar e acessar transições com suas respectivas prioridades.\n",
        "\n",
        "2. **Atualização da Memória**: Durante a coleta de experiências, cada transição é armazenada na memória com uma prioridade inicial alta para garantir que seja amostrada pelo menos uma vez.\n",
        "\n",
        "3. **Amostragem Prioritária**: Durante o treinamento, transições são amostradas com uma probabilidade proporcional à sua prioridade. Isso garante que transições mais importantes tenham mais chances de serem selecionadas.\n",
        "\n",
        "4. **Rebalanceamento**: Para evitar que apenas transições de alta prioridade sejam amostradas, uma técnica de balanceamento é usada para garantir que todas as transições tenham uma chance mínima de serem selecionadas, promovendo uma exploração mais ampla do espaço de estados.\n",
        "\n",
        "5. **Atualização das Prioridades**: Após cada atualização da rede, o TD error das transições amostradas é recalculado e suas prioridades na memória de replay são ajustadas de acordo.\n",
        "\n",
        "### Vantagens:\n",
        "\n",
        "- **Eficiência do Aprendizado**: Ao focar em transições mais informativas, o PER acelera o processo de aprendizado e melhora a performance do agente.\n",
        "- **Exploração e Exploração Balanceadas**: A técnica de rebalanceamento garante que o agente não negligencie completamente transições menos importantes, mantendo um bom equilíbrio entre exploração e exploração.\n",
        "\n",
        "### Aplicações:\n",
        "\n",
        "- **Jogos de Vídeo**: O PER pode ser aplicado em algoritmos como o DQN para melhorar a performance em ambientes de alta dimensionalidade, como jogos.\n",
        "- **Robótica e Automação**: Pode ser usado para otimizar o aprendizado de robôs em tarefas complexas.\n",
        "- **Problemas de Decisão e Controle**: Em áreas como finanças, saúde e logística, onde o aprendizado eficiente é crucial.\n",
        "\n",
        "O PER representa uma abordagem mais sofisticada para o gerenciamento de experiências em algoritmos de Aprendizado por Reforço, aumentando a eficiência do treinamento e permitindo que o agente aprenda políticas eficazes mais rapidamente."
      ],
      "metadata": {
        "id": "ZpF7f_8d33hw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13qYpvBD2oZh"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8mRbjEO2sof"
      },
      "outputs": [],
      "source": [
        "reducao = \"none\"                # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V2_rb\"  # Define o nome da experiência.\n",
        "etiqueta = \"video_V2_rb\"        # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Agente V2"
      ],
      "metadata": {
        "id": "eg8hJFJ4lHrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V2_RB:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_Amostra_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao             # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "\n",
        "            for passo_atual in dado[\"next\",\"step_count\"]:\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "            indices = self.rb.extend(dado)                                     # Grava dados no buffer de repetição\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay, acessando o campo \"step_count\".\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info=True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      valores_erro[\"loss\"] = valores_erro[\"loss\"].mean()\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                              # Atualiza o contador total de passos coletados.\n",
        "            self.total_episodios += dado[\"next\", \"done\"].sum()             # Atualiza o contador total de episódios completados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if self.total_episodios >= self.episodios:\n",
        "                break\n",
        "\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "MWBx22aVlPAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "iV0jQ22aofx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V2_RB = Mod_Agente_V2_RB(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                 passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau,\n",
        "                                 caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V2_RB.treinar_agente(episodios, tamanho_amostra)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V2_RB.ambiente, agente_V2_RB.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V2_rb\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "9pCA8k2AoiOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9lcka8E-V4K"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Doy0Lppb-V4K"
      },
      "outputs": [],
      "source": [
        "resultado_1_V2_rb = carrega_array(\"C1\",\n",
        "                  \"V2_rb\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V2_rb.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5LseCMvggzU"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V2_rb\",\n",
        "                           inicio = 1,\n",
        "                           fim = 20,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU4dAEZ9-V4K"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V2_rb = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da-00ehHC9NM"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_rb['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZXEbD76DNur"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_rb['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChF1_jnTDRS0"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_rb['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrxnGRxKDU-s"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_rb['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_MdZGcRDYxb"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_rb['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estatísticas"
      ],
      "metadata": {
        "id": "f0ssHDNFjysw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media_recompensa_treinamento_V2_rb = np.mean(resultado_medio_V2_rb['Rewards_ep'])\n",
        "print(f\"A recompensa média do treinamento é: {media_recompensa_treinamento_V2_rb}\")\n",
        "\n",
        "media_recompensa_avaliacao_V2_rb = np.mean(resultado_medio_V2_rb['Rewards_ep_eval'])\n",
        "print(f\"A recompensa média da avaliação é: {media_recompensa_avaliacao_V2_rb}\")\n",
        "\n",
        "media_taxa_sucesso_V2_rb = np.mean(resultado_medio_V2_rb['Sucess_Rates'])\n",
        "print(f\"A taxa de sucesso média é: {media_taxa_sucesso_V2_rb}\")"
      ],
      "metadata": {
        "id": "mq0uqdvkj00U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9EeR8i_qrAD"
      },
      "source": [
        "# V2.1: PER Priorização Média"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Descrição"
      ],
      "metadata": {
        "id": "sFc9rNS44EhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A PER média amostral é similar à PER tradicional, porém, em vez de priorizar cada amostra individualmente, as amostras são agrupadas em conjuntos. A priorização de cada conjunto é determinada pela média das prioridades individuais das experiências que o compõem."
      ],
      "metadata": {
        "id": "iLcRhi4A4He2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kimGwSNqmzP"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hyeNwDDqr2P"
      },
      "outputs": [],
      "source": [
        "reducao = \"mean\"                # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V2.1\"   # Define o nome da experiência.\n",
        "etiqueta = \"video_V2.1\"         # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Agente V2.1"
      ],
      "metadata": {
        "id": "9B8AqHaj3EYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V2_1:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao            # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "\n",
        "            for passo_atual in dado[\"next\",\"step_count\"]:\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "            indices = self.rb.extend(dado)                                     # Grava dados no buffer de repetição\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay, acessando o campo \"step_count\".\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info=True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                              # Atualiza o contador total de passos coletados.\n",
        "            self.total_episodios += dado[\"next\", \"done\"].sum()             # Atualiza o contador total de episódios completados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if self.total_episodios >= self.episodios:\n",
        "                break\n",
        "\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "_YFb70zT3JzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "rOWuYvahCbzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V2_1 = Mod_Agente_V2_1(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                  passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau,\n",
        "                                  caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V2_1.treinar_agente(episodios, tamanho_amostra)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V2_1.ambiente, agente_V2_1.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V2.1\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "REsFvOIXCej5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL5EopQjqrAf"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iWGnCPOqrAf"
      },
      "outputs": [],
      "source": [
        "resultado_1_V2_1 = carrega_array(\"C1\",\n",
        "                  \"V2.1\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V2_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJNQ127gqrAf"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V2.1\",\n",
        "                           inicio = 1,\n",
        "                           fim = 20,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyKMW20gqrAf"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V2_1 = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8Ux9u1pqrAf"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_1['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzlaTalIqrAg"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_1['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-YzPMERqrAg"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_1['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2lbzhm5MwIH"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_1['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXVz2whUMyqg"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_1['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estatísticas"
      ],
      "metadata": {
        "id": "RqI58fZFk9cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media_recompensa_treinamento_V2_1 = np.mean(resultado_medio_V2_1['Rewards_ep'])\n",
        "print(f\"A recompensa média do treinamento é: {media_recompensa_treinamento_V2_1}\")\n",
        "\n",
        "media_recompensa_avaliacao_V2_1 = np.mean(resultado_medio_V2_1['Rewards_ep_eval'])\n",
        "print(f\"A recompensa média da avaliação é: {media_recompensa_avaliacao_V2_1}\")\n",
        "\n",
        "media_taxa_sucesso_V2_1 = np.mean(resultado_medio_V2_1['Sucess_Rates'])\n",
        "print(f\"A taxa de sucesso média é: {media_taxa_sucesso_V2_1}\")"
      ],
      "metadata": {
        "id": "Q7C9v4TLlAFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOT3XF37NKjj"
      },
      "source": [
        "# V2.2: PER Priorização Somada"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descrição"
      ],
      "metadata": {
        "id": "jLT3Qs8d4c8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A PER soma amostral é semelhante à PER média amostral, com a diferença de que, em vez de calcular a média das prioridades das experiências que compõem o conjunto, realiza-se a soma das prioridades individuais. O valor resultante é então aplicado a todo o conjunto."
      ],
      "metadata": {
        "id": "li6EOqsE4e6Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8QaiphINKjv"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PCw_5C-NKjw"
      },
      "outputs": [],
      "source": [
        "reducao = \"sum\"                   # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V2.2\"     # Define o nome da experiência.\n",
        "etiqueta = \"video_V2.2\"           # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Agente V2.2"
      ],
      "metadata": {
        "id": "pbrcwVuQEE00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V2_2:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_Amostra_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao            # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "\n",
        "            for passo_atual in dado[\"next\",\"step_count\"]:\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "            indices = self.rb.extend(dado)                                     # Grava dados no buffer de repetição\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay, acessando o campo \"step_count\".\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info=True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                              # Atualiza o contador total de passos coletados.\n",
        "            self.total_episodios += dado[\"next\", \"done\"].sum()             # Atualiza o contador total de episódios completados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if self.total_episodios >= self.episodios:\n",
        "                break\n",
        "\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "Q_s95sqnEJW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "iCavwONiEaf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V2_2 = Mod_Agente_V2_2(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                  passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau,\n",
        "                                  caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V2_2.treinar_agente(episodios, tamanho_amostra)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V2_2.ambiente, agente_V2_2.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V2.2\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "lmM-hDOBEh7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ojpAizPNKj4"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzJclPbtNKj4"
      },
      "outputs": [],
      "source": [
        "resultado_1_V2_2 = carrega_array(\"C1\",\n",
        "                  \"V2.2\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V2_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PJAie4TNKj4"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V2.2\",\n",
        "                           inicio = 1,\n",
        "                           fim = 20,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkCUnVaVNKj4"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V2_2 = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAEG-bgDNKj5"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_2['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_J-GU57NKj5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_2['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw04A_k3NKj5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_2['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH8gP5KONKj5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_2['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-RpzmuMNKj6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V2_2['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estatísticas"
      ],
      "metadata": {
        "id": "UYaRHbzsl_M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media_recompensa_treinamento_V2_2 = np.mean(resultado_medio_V2_2['Rewards_ep'])\n",
        "print(f\"A recompensa média do treinamento é: {media_recompensa_treinamento_V2_2}\")\n",
        "\n",
        "media_recompensa_avaliacao_V2_2 = np.mean(resultado_medio_V2_2['Rewards_ep_eval'])\n",
        "print(f\"A recompensa média da avaliação é: {media_recompensa_avaliacao_V2_2}\")\n",
        "\n",
        "media_taxa_sucesso_V2_2 = np.mean(resultado_medio_V2_2['Sucess_Rates'])\n",
        "print(f\"A taxa de sucesso média é: {media_taxa_sucesso_V2_2}\")"
      ],
      "metadata": {
        "id": "HsFFvwr0mBf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODHBnEPFNKj6"
      },
      "source": [
        "## Renderização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14CxShO-NKj6"
      },
      "source": [
        "Por fim, executamos o ambiente em tantas etapas quanto possível e salvamos o vídeo localmente (observe que não estamos explorando)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SKPzqSfNKj6"
      },
      "outputs": [],
      "source": [
        "# Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "record_env.rollout(max_steps=1000, policy=policy)\n",
        "\n",
        "# Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "video_recorder.dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta2Lr4DRNKj6"
      },
      "outputs": [],
      "source": [
        "# logger # CSVLogger(exp_name=dqn, experiment=CSVExperiment(log_dir=./training_loop/dqn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vk9VV-1mVXt"
      },
      "source": [
        "# V3: PER Últimos Índices (Priorização 1ª Vez)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descrição"
      ],
      "metadata": {
        "id": "xD0JHmU46NTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semelhante ao PER tradicional, porém as amostras recebem um incremento a mais no seus últimos índices."
      ],
      "metadata": {
        "id": "UWykUApq6P36"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohkj6RMCmVX6"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxwFVNWBmVX6"
      },
      "outputs": [],
      "source": [
        "reducao = \"none\"                  # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V3_1v\"    # Define o nome da experiência.\n",
        "etiqueta = \"video_V3_1v\"          # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Agente V3"
      ],
      "metadata": {
        "id": "mcRJ-vdrl9zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V3_1v:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_Amostra_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao            # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra, recompensa_maxima):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "        self.recompensa_maxima = recompensa_maxima        # Especifica a recompensa máxima que o agente pode atingir.\n",
        "\n",
        "        ### CONTROLE DO TREINAMENTO ###\n",
        "        Num_do_Teste               = 9\n",
        "        Quantidade_de_Treinamentos = 2\n",
        "        Criterio_de_Parada         = 5\n",
        "        Parada                     = False\n",
        "        Avaliacao_no_Treinamento   = False\n",
        "        Log_Estatico_dos_Result    = True\n",
        "\n",
        "        ### VARIÁVEIS UTILIZADAS ###\n",
        "        parada_500                = 0\n",
        "        indice_final            = 0\n",
        "        mean_recomp_ep          = []\n",
        "        # indices_incrementar     = []\n",
        "        parar                    = False\n",
        "        Valor_Anterior          = None\n",
        "        indices_incrementar     = deque(maxlen = 10_000)\n",
        "        incremento_prioridade   = torch.full((len(indices_incrementar),), 10)\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "            indices = self.rb.extend(dado)      # Grava dados no buffer de repetição\n",
        "\n",
        "            for l, passo_atual in enumerate(dado[\"next\",\"step_count\"]):\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.total_episodios += 1         # Atualiza o contador total de episódios completados.\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                        # ########################## MUDANÇA IMPLEMENTADA ########################\n",
        "                        indices_incrementar.append(indices[l-1].item())\n",
        "                        incremento_prioridade = torch.full((len(indices_incrementar),), 100)\n",
        "                        self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)            # 1ª VEZ\n",
        "                        # ########################################################################\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "                # if passo_atual == self.recompensa_maxima:\n",
        "                # ############## CONDIÇÕES DE PARADA ################\n",
        "                #     parada_500 += 1\n",
        "                #     if Parada:\n",
        "                #         if parada_500 >= Criterio_de_Parada: stop = not stop\n",
        "                # if Avaliacao_no_Treinamento:\n",
        "                #     recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao = avaliar(self.ambiente,\n",
        "                #                                                                                                 self.politica_deterministica,\n",
        "                #                                                                                                 max_steps = passos_maximo)\n",
        "                #     if media_recompensas_avaliacao >= (self.recompensa_maxima * 0.99): parar = not parar\n",
        "                ###################################################\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if ((self.total_episodios >= self.episodios) and (Parada == False)): break\n",
        "            elif parar: break\n",
        "\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay.\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info = True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      valores_erro[\"loss\"] = valores_erro[\"loss\"].mean()      # Usar PER Normal pois reduction = \"None\"\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                          # Atualiza o contador total de passos coletados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "            # if (Log_Estatico_dos_Result): clear_output(wait=True)\n",
        "            # Num_Treinamentos += 1\n",
        "            # Num_do_Teste += 1\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "ioYxd5HvmIcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "LWocUGIOn32o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V3_1v = Mod_Agente_V3_1v(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                  passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau,\n",
        "                                  caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V3_1v.treinar_agente(episodios, tamanho_amostra, recompensa_maxima)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V3_1v.ambiente, agente_V3_1v.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V3_1v\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "rfXTHPIrn61E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFoXxQemmVYJ"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28d0VFm6mVYK"
      },
      "outputs": [],
      "source": [
        "resultado_1_V3_1v = carrega_array(\"C1\",\n",
        "                  \"V3_1v\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V3_1v.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3MaUj9MmVYK"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V3_1v\",\n",
        "                           inicio = 1,\n",
        "                           fim = 20,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQYxgMpvmVYK"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V3_1v = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xirt5P3mVYL"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_1v['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW8KKqHBmVYL"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_1v['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ub_NyUiDmVYM"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_1v['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0p4361iOmVYM"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_1v['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JOxRXITmmVYN"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_1v['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estatísticas"
      ],
      "metadata": {
        "id": "13p9Xfz8nXLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media_recompensa_treinamento_V3_1v = np.mean(resultado_medio_V3_1v['Rewards_ep'])\n",
        "print(f\"A recompensa média do treinamento é: {media_recompensa_treinamento_V3_1v}\")\n",
        "\n",
        "media_recompensa_avaliacao_V3_1v = np.mean(resultado_medio_V3_1v['Rewards_ep_eval'])\n",
        "print(f\"A recompensa média da avaliação é: {media_recompensa_avaliacao_V3_1v}\")\n",
        "\n",
        "media_taxa_sucesso_V3_1v = np.mean(resultado_medio_V3_1v['Sucess_Rates'])\n",
        "print(f\"A taxa de sucesso média é: {media_taxa_sucesso_V3_1v}\")"
      ],
      "metadata": {
        "id": "DIcquIWmnZOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhM98GsKJLSm"
      },
      "source": [
        "# V3.1: PER Melhores Índices (Max)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descrição"
      ],
      "metadata": {
        "id": "-YGN2Bm8_mXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semelhante ao PER útimos índices, porém os últimos índices só são priorizados quando a pontuação máxima é atingida, o que leva ao nome de melhores índices."
      ],
      "metadata": {
        "id": "iudJdTS8_oJ_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u94Ym_3sJLSx"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8y6vPdrJLSy"
      },
      "outputs": [],
      "source": [
        "reducao = \"none\"          # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V3.1\"            # Define o nome da experiência.\n",
        "etiqueta = \"video_V3.1\"               # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe V3.1"
      ],
      "metadata": {
        "id": "USUOW5nkJLSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V3_1:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_Amostra_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao            # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra, recompensa_maxima):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "        self.recompensa_maxima = recompensa_maxima        # Especifica a recompensa máxima que o agente pode atingir.\n",
        "\n",
        "        ### CONTROLE DO TREINAMENTO ###\n",
        "        Num_do_Teste               = 9\n",
        "        Quantidade_de_Treinamentos = 2\n",
        "        Criterio_de_Parada         = 5\n",
        "        Parada                     = False\n",
        "        Avaliacao_no_Treinamento   = False\n",
        "        Log_Estatico_dos_Result    = True\n",
        "\n",
        "        ### VARIÁVEIS UTILIZADAS ###\n",
        "        parada_500                = 0\n",
        "        indice_final            = 0\n",
        "        mean_recomp_ep          = []\n",
        "        # indices_incrementar     = []\n",
        "        parar                    = False\n",
        "        Valor_Anterior          = None\n",
        "        indices_incrementar     = deque(maxlen = 10_000)\n",
        "        incremento_prioridade   = torch.full((len(indices_incrementar),), 10)\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "            indices = self.rb.extend(dado)      # Grava dados no buffer de repetição\n",
        "\n",
        "            for l, passo_atual in enumerate(dado[\"next\",\"step_count\"]):\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.total_episodios += 1         # Atualiza o contador total de episódios completados.\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "                if passo_atual == self.recompensa_maxima:\n",
        "                # ############## CONDIÇÕES DE PARADA ################\n",
        "                #     parada_500 += 1\n",
        "                #     if Parada:\n",
        "                #         if parada_500 >= Criterio_de_Parada: stop = not stop\n",
        "                # if Avaliacao_no_Treinamento:\n",
        "                #     recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao = avaliar(self.ambiente,\n",
        "                #                                                                                                 self.politica_deterministica,\n",
        "                #                                                                                                 max_steps = passos_maximo)\n",
        "                #     if media_recompensas_avaliacao >= (self.recompensa_maxima * 0.99): parar = not parar\n",
        "                ###################################################\n",
        "                # ########################## MUDANÇA IMPLEMENTADA ########################\n",
        "                    indices_incrementar.append(max(dado[\"next\",\"step_count\"]).item())\n",
        "                    incremento_prioridade = torch.full((len(indices_incrementar),), 100)\n",
        "                    self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)\n",
        "                # ########################################################################\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if ((self.total_episodios >= self.episodios) and (Parada == False)): break\n",
        "            elif parar: break\n",
        "\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay.\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info = True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      # self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)                  # 2ª VEZ\n",
        "                      valores_erro[\"loss\"] = valores_erro[\"loss\"].mean()      # Usar PER Normal pois reduction = \"None\"\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                          # Atualiza o contador total de passos coletados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "            # if (Log_Estatico_dos_Result): clear_output(wait=True)\n",
        "            # Num_Treinamentos += 1\n",
        "            # Num_do_Teste += 1\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "HYaD1HiRJLSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "cEjlYQTOJLSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V3_1 = Mod_Agente_V3_1(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                  passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau,\n",
        "                                  caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V3_1.treinar_agente(episodios, tamanho_amostra, recompensa_maxima)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V3_1.ambiente, agente_V3_1.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V3_1\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "ioyuDTLpJLSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4P1JCKTJLSz"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssumH8vuJLSz"
      },
      "outputs": [],
      "source": [
        "resultado_1_V3_1 = carrega_array(\"C1\",\n",
        "                  \"V3_1\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V3_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsNx0lx_JLS0"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V3_1\",\n",
        "                           inicio = 1,\n",
        "                           fim = 10,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxPzpqe1JLS1"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V3_1 = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZKsI0WKsJLS1"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_1['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OaH1perAJLS1"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_1['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5vDelE6IJLS2"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_1['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "anghkzHbJLS2"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_1['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S6EDgu91JLS2"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_1['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y42T5lnnJLS2"
      },
      "source": [
        "# V3.2: PER Melhores Índices (Semi-Trajetória 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descrição"
      ],
      "metadata": {
        "id": "MiCqB0ejAkzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semelhante ao PER melhores índices, porém  não são salvos apenas os últimos índices, são salvos um conjunto de índices (semi-trajetória). Quando a pontuação máxima é atingida, a semi-trajetória será formada pelos índices que atingiram um percentual maior que a base definida em relação a pontuação máxima."
      ],
      "metadata": {
        "id": "HRoA8FB7AqNx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8u1VtNJLS2"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh-_HSj0JLS3"
      },
      "outputs": [],
      "source": [
        "reducao = \"none\"          # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V3.2_sm1\"            # Define o nome da experiência.\n",
        "etiqueta = \"video_V3.2_sm1\"               # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Agente V3.2"
      ],
      "metadata": {
        "id": "tfl__DCzJLS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V3_2_sm1:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_Amostra_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao            # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra, recompensa_maxima):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "        self.recompensa_maxima = recompensa_maxima        # Especifica a recompensa máxima que o agente pode atingir.\n",
        "\n",
        "        ### CONTROLE DO TREINAMENTO ###\n",
        "        Num_do_Teste               = 9\n",
        "        Quantidade_de_Treinamentos = 2\n",
        "        Criterio_de_Parada         = 5\n",
        "        Parada                     = False\n",
        "        Avaliacao_no_Treinamento   = False\n",
        "        Log_Estatico_dos_Result    = True\n",
        "\n",
        "        ### VARIÁVEIS UTILIZADAS ###\n",
        "        parada_500                = 0\n",
        "        indice_final            = 0\n",
        "        mean_recomp_ep          = []\n",
        "        # indices_incrementar     = []\n",
        "        parar                    = False\n",
        "        Valor_Anterior          = None\n",
        "        indices_incrementar     = deque(maxlen = 10_000)\n",
        "        incremento_prioridade   = torch.full((len(indices_incrementar),), 10)\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "            indices = self.rb.extend(dado)      # Grava dados no buffer de repetição\n",
        "\n",
        "            for l, passo_atual in enumerate(dado[\"next\",\"step_count\"]):\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.total_episodios += 1         # Atualiza o contador total de episódios completados.\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "                if passo_atual == self.recompensa_maxima:\n",
        "                # ############## CONDIÇÕES DE PARADA ################\n",
        "                #     parada_500 += 1\n",
        "                #     if Parada:\n",
        "                #         if parada_500 >= Criterio_de_Parada: stop = not stop\n",
        "                # if Avaliacao_no_Treinamento:\n",
        "                #     recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao = avaliar(self.ambiente,\n",
        "                #                                                                                                 self.politica_deterministica,\n",
        "                #                                                                                                 max_steps = passos_maximo)\n",
        "                #     if media_recompensas_avaliacao >= (recompensa_maxima * 0.99): parar = not parar\n",
        "                ###################################################\n",
        "                # ########################## MUDANÇA IMPLEMENTADA ############################\n",
        "                    melhores_indicies = torch.nonzero(dado[\"next\",\"step_count\"] > (self.recompensa_maxima * 0.6), as_tuple = True)[0]\n",
        "                    for i in melhores_indicies.tolist():\n",
        "                        indices_incrementar.append(indices[i].item())\n",
        "                    incremento_prioridade = torch.full((len(indices_incrementar),), 10)\n",
        "                    self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)\n",
        "                # ############################################################################\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if ((self.total_episodios >= self.episodios) and (Parada == False)): break\n",
        "            elif parar: break\n",
        "\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay.\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info = True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      # self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)                  # 2ª VEZ\n",
        "                      valores_erro[\"loss\"] = valores_erro[\"loss\"].mean()      # Usar PER Normal pois reduction = \"None\"\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                          # Atualiza o contador total de passos coletados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "            # if (Log_Estatico_dos_Result): clear_output(wait=True)\n",
        "            # Num_Treinamentos += 1\n",
        "            # Num_do_Teste += 1\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "ct2SQXObJLS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "cfVqvIaCJLS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V3_2_sm1 = Mod_Agente_V3_2_sm1(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                  passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau,\n",
        "                                  caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V3_2_sm1.treinar_agente(episodios, tamanho_amostra, recompensa_maxima)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V3_2_sm1.ambiente, agente_V3_2_sm1.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V3.2_sm1\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "8tZpjzGjJLS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvQNXJZjJLS4"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDLUX2sBJLS4"
      },
      "outputs": [],
      "source": [
        "resultado_1_V3_2_sm1 = carrega_array(\"C1\",\n",
        "                  \"V3.2_sm1\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V3_2_sm1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXTKFtlFJLS4"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V3.2_sm1\",\n",
        "                           inicio = 1,\n",
        "                           fim = 10,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rgFAJAdJLS5"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V3_2_sm1 = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Fl8TTEL4JLS5"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_2_sm1['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3LOeJpRkJLS5"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_2_sm1['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qA7GRHtmJLS5"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_2_sm1['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aGj6uRnPJLS5"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_2_sm1['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "as9BYDpqJLS6"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_2_sm1['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2qZ5hneXNf7"
      },
      "source": [
        "# V3.3: PER Piores Índices (Max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnztUYPKXNgL"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYBf9poLXNgL"
      },
      "outputs": [],
      "source": [
        "reducao = \"none\"          # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V3.3\"             # Define o nome da experiência.\n",
        "etiqueta = \"video_V3.3\"             # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe V3.3"
      ],
      "metadata": {
        "id": "ja0KjdT9kS0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V3_3:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_Amostra_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao            # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra, recompensa_maxima):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "        self.recompensa_maxima = recompensa_maxima        # Especifica a recompensa máxima que o agente pode atingir.\n",
        "\n",
        "        ### CONTROLE DO TREINAMENTO ###\n",
        "        Num_do_Teste               = 9\n",
        "        Quantidade_de_Treinamentos = 2\n",
        "        Criterio_de_Parada         = 5\n",
        "        Parada                     = False\n",
        "        Avaliacao_no_Treinamento   = False\n",
        "        Log_Estatico_dos_Result    = True\n",
        "\n",
        "        ### VARIÁVEIS UTILIZADAS ###\n",
        "        parada_500                = 0\n",
        "        indice_final            = 0\n",
        "        mean_recomp_ep          = []\n",
        "        # indices_incrementar     = []\n",
        "        parar                    = False\n",
        "        Valor_Anterior          = None\n",
        "        indices_incrementar     = deque(maxlen = 10_000)\n",
        "        incremento_prioridade   = torch.full((len(indices_incrementar),), 10)\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "            indices = self.rb.extend(dado)      # Grava dados no buffer de repetição\n",
        "\n",
        "            for l, passo_atual in enumerate(dado[\"next\",\"step_count\"]):\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.total_episodios += 1         # Atualiza o contador total de episódios completados.\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "                if passo_atual < (self.recompensa_maxima * 0.1):\n",
        "                # ########################## MUDANÇA IMPLEMENTADA ########################\n",
        "                    indices_incrementar.append(max(dado[\"next\",\"step_count\"]).item())\n",
        "                    incremento_prioridade = torch.full((len(indices_incrementar),), 100)\n",
        "                    self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)\n",
        "                # ########################################################################\n",
        "\n",
        "                # if passo_atual == self.recompensa_maxima:\n",
        "                # ############## CONDIÇÕES DE PARADA ################\n",
        "                #     parada_500 += 1\n",
        "                #     if Parada:\n",
        "                #         if parada_500 >= Criterio_de_Parada: stop = not stop\n",
        "                # if Avaliacao_no_Treinamento:\n",
        "                #     recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao = avaliar(self.ambiente,\n",
        "                #                                                                                                 self.politica_deterministica,\n",
        "                #                                                                                                 max_steps = passos_maximo)\n",
        "                #     if media_recompensas_avaliacao >= (self.recompensa_maxima * 0.99): parar = not parar\n",
        "                ###################################################\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if ((self.total_episodios >= self.episodios) and (Parada == False)): break\n",
        "            elif parar: break\n",
        "\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay.\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info = True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      # self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)                  # 2ª VEZ\n",
        "                      valores_erro[\"loss\"] = valores_erro[\"loss\"].mean()      # Usar PER Normal pois reduction = \"None\"\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                          # Atualiza o contador total de passos coletados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "            # if (Log_Estatico_dos_Result): clear_output(wait=True)\n",
        "            # Num_Treinamentos += 1\n",
        "            # Num_do_Teste += 1\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "F8g16kcUkYTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "M96aa3WHwT6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V3_3 = Mod_Agente_V3_3(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                  passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau,\n",
        "                                  caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V3_3.treinar_agente(episodios, tamanho_amostra, recompensa_maxima)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V3_3.ambiente, agente_V3_3.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V3_3\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "0ZNQwuMkwion"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTNa9Qh9XNgQ"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgoItqMIXNgQ"
      },
      "outputs": [],
      "source": [
        "resultado_1_V3_3 = carrega_array(\"C1\",\n",
        "                  \"V3_3\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V3_3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKvKyQvwXNgQ"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V3_3\",\n",
        "                           inicio = 1,\n",
        "                           fim = 10,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jFtoT2FXNgR"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V3_3 = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NCMTWJPqXNgR"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_3['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "18F7bcgRXNgR"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_3['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eFVPGCBSXNgR"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_3['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xyyPqpZkXNgR"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_3['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mN7KmZC8XNgS"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_3['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j7nn3Nw1I1j"
      },
      "source": [
        "# V3.4: PER Piores Índices (Semi-Trajetória 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descrição"
      ],
      "metadata": {
        "id": "P65_oK1gDi42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semelhante ao PER melhores índices (semi-trajetória), porém trabalha-se com um percentual baixo afim de pegar os piores índices. Essa abordagem visa trabalhar na base do desafio, fazendo com que o agente aprenda bastante os primeiros passos e daí desenvolva o resto do desafio."
      ],
      "metadata": {
        "id": "1hxaoODhDl4k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5gVmauJ1I1v"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8vNWIPM1I1v"
      },
      "outputs": [],
      "source": [
        "reducao = \"none\"          # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V3.4_sm2\"            # Define o nome da experiência.\n",
        "etiqueta = \"video_V3.4_sm2\"               # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Agente V3.4"
      ],
      "metadata": {
        "id": "eUTd7hudDX7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V3_4_sm2:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, funcao_perda, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_Amostra_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao            # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra, recompensa_maxima):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "        self.recompensa_maxima = recompensa_maxima        # Especifica a recompensa máxima que o agente pode atingir.\n",
        "\n",
        "        ### CONTROLE DO TREINAMENTO ###\n",
        "        Num_do_Teste               = 9\n",
        "        Quantidade_de_Treinamentos = 2\n",
        "        Criterio_de_Parada         = 5\n",
        "        Parada                     = False\n",
        "        Avaliacao_no_Treinamento   = False\n",
        "        Log_Estatico_dos_Result    = True\n",
        "\n",
        "        ### VARIÁVEIS UTILIZADAS ###\n",
        "        parada_500                = 0\n",
        "        indice_final            = 0\n",
        "        mean_recomp_ep          = []\n",
        "        # indices_incrementar     = []\n",
        "        parar                    = False\n",
        "        Valor_Anterior          = None\n",
        "        indices_incrementar     = deque(maxlen = 10_000)\n",
        "        incremento_prioridade   = torch.full((len(indices_incrementar),), 10)\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "            indices = self.rb.extend(dado)      # Grava dados no buffer de repetição\n",
        "\n",
        "            for l, passo_atual in enumerate(dado[\"next\",\"step_count\"]):\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.total_episodios += 1         # Atualiza o contador total de episódios completados.\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                # ########################## MUDANÇA IMPLEMENTADA ############################\n",
        "                if passo_atual < (self.recompensa_maxima * 0.1):\n",
        "                    indices_incrementar.append(indices[l].item())\n",
        "                    incremento_prioridade = torch.full((len(indices_incrementar),), 10)\n",
        "                    self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)\n",
        "                # ############################################################################\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "                # if passo_atual == self.recompensa_maxima:\n",
        "                # ############## CONDIÇÕES DE PARADA ################\n",
        "                #     parada_500 += 1\n",
        "                #     if Parada:\n",
        "                #         if parada_500 >= Criterio_de_Parada: stop = not stop\n",
        "                # if Avaliacao_no_Treinamento:\n",
        "                #     recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao = avaliar(self.ambiente,\n",
        "                #                                                                                                 self.politica_deterministica,\n",
        "                #                                                                                                 max_steps = passos_maximo)\n",
        "                #     if media_recompensas_avaliacao >= (recompensa_maxima * 0.99): parar = not parar\n",
        "                ###################################################\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if ((self.total_episodios >= self.episodios) and (Parada == False)): break\n",
        "            elif parar: break\n",
        "\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay.\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info = True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      # self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)                  # 2ª VEZ\n",
        "                      valores_erro[\"loss\"] = valores_erro[\"loss\"].mean()      # Usar PER Normal pois reduction = \"None\"\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                          # Atualiza o contador total de passos coletados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "            # if (Log_Estatico_dos_Result): clear_output(wait=True)\n",
        "            # Num_Treinamentos += 1\n",
        "            # Num_do_Teste += 1\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "qCtG5on2Dbdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "Ry-Qe9SfDjGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V3_4_sm2 = Mod_Agente_V3_4_sm2(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                  passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, taxa_aprendizado, tau,\n",
        "                                  caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V3_4_sm2.treinar_agente(episodios, tamanho_amostra, recompensa_maxima)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V3_4_sm2.ambiente, agente_V3_4_sm2.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V3.4_sm2\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "_LNOPGFEDlgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arg4AIoU1I1z"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojlUis-x1I10"
      },
      "outputs": [],
      "source": [
        "resultado_1_V3_4_sm2 = carrega_array(\"C1\",\n",
        "                  \"V3.4_sm2\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V3_4_sm2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JyrKORd1I10"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V3.4_sm2\",\n",
        "                           inicio = 1,\n",
        "                           fim = 10,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pdtmg5Hd1I10"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V3_4_sm2 = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UG_hhd0A1I11"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_4_sm2['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NsIXgQoA1I11"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_4_sm2['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C40rSKjr1I11"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_4_sm2['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xgpkL-TZ1I11"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_4_sm2['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FTiUg-Bm1I11"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V3_4_sm2['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RobDUuLaZrg8"
      },
      "source": [
        "# V4: PER Última Trajetória"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descrição"
      ],
      "metadata": {
        "id": "ZDBw73AwFOgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semelhante as versões de PER índices, porém não são salvos apenas uma quantidade índices, mas todos os índices do episódio (trajetória), quando a pontuação máxima é atingida."
      ],
      "metadata": {
        "id": "Dpv3y5i0FY6E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMLdhNj9ZrhN"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9F3_fWtZrhO"
      },
      "outputs": [],
      "source": [
        "reducao = \"none\"          # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V4\"            # Define o nome da experiência.\n",
        "etiqueta = \"video_V4\"               # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Agente V4"
      ],
      "metadata": {
        "id": "gUyVqm8rZrhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V4:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_Amostra_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao            # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra, recompensa_maxima):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "        self.recompensa_maxima = recompensa_maxima        # Especifica a recompensa máxima que o agente pode atingir.\n",
        "\n",
        "        ### CONTROLE DO TREINAMENTO ###\n",
        "        Num_do_Teste               = 9\n",
        "        Quantidade_de_Treinamentos = 2\n",
        "        Criterio_de_Parada         = 5\n",
        "        Parada                     = False\n",
        "        Avaliacao_no_Treinamento   = False\n",
        "        Log_Estatico_dos_Result    = True\n",
        "\n",
        "        ### VARIÁVEIS UTILIZADAS ###\n",
        "        parada_500                = 0\n",
        "        indice_final              = 0\n",
        "        mean_recomp_ep            = []\n",
        "        # indices_incrementar     = []\n",
        "        parar                    = False\n",
        "        Valor_Anterior          = None\n",
        "        indices_incrementar     = deque(maxlen = 10_000)\n",
        "        incremento_prioridade   = torch.full((500,), 10)\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "            indices = self.rb.extend(dado)      # Grava dados no buffer de repetição\n",
        "\n",
        "            for l, passo_atual in enumerate(dado[\"next\",\"step_count\"]):\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.total_episodios += 1         # Atualiza o contador total de episódios completados.\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "                if passo_atual == self.recompensa_maxima:\n",
        "                # ############## CONDIÇÕES DE PARADA ################\n",
        "                #     parada_500 += 1\n",
        "                #     if Parada:\n",
        "                #         if parada_500 >= Criterio_de_Parada: stop = not stop\n",
        "                # if Avaliacao_no_Treinamento:\n",
        "                #     recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao = avaliar(self.ambiente,\n",
        "                #                                                                                                 self.politica_deterministica,\n",
        "                #                                                                                                 max_steps = passos_maximo)\n",
        "                #     if media_recompensas_avaliacao >= (recompensa_maxima * 0.99): parar = not parar\n",
        "                ###################################################\n",
        "                ############################ MUDANÇA IMPLEMENTADA ########################################\n",
        "                    indice_final = indices[-1]\n",
        "                    indice_500 = torch.nonzero(dado[\"next\",\"step_count\"] == 499, as_tuple=True)\n",
        "                    indices_incrementar = torch.arange(indice_final.item()+1 - 499 + indice_500[0].item()-499,\n",
        "                                                  indice_final.item()+1 - 499 + indice_500[0].item()+1)\n",
        "                    self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)\n",
        "                ##########################################################################################\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if ((self.total_episodios >= self.episodios) and (Parada == False)): break\n",
        "            elif parar: break\n",
        "\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay.\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info = True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      # self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)                  # 2ª VEZ\n",
        "                      valores_erro[\"loss\"] = valores_erro[\"loss\"].mean()      # Usar PER Normal pois reduction = \"None\"\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                          # Atualiza o contador total de passos coletados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "            # if (Log_Estatico_dos_Result): clear_output(wait=True)\n",
        "            # Num_Treinamentos += 1\n",
        "            # Num_do_Teste += 1\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "Jh6m9j7BZrhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "tGPIKtvtZrhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V4 = Mod_Agente_V4(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                  passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau,\n",
        "                                  caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V4.treinar_agente(episodios, tamanho_amostra, recompensa_maxima)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V4.ambiente, agente_V4.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V4\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "aLtjUX3aZrhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sOouQkfZrhP"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3KeiyiNZrhP"
      },
      "outputs": [],
      "source": [
        "resultado_1_V4 = carrega_array(\"C1\",\n",
        "                  \"V4\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jzls5naZrhP"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V4\",\n",
        "                           inicio = 1,\n",
        "                           fim = 20,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa4w_nPMZrhP"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V4 = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuE9JQUwZrhP"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KFu66X61ZrhP"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-eXtMBIEZrhP"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w4AFhbQhZrhP"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oQn14L7KZrhQ"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estatísticas"
      ],
      "metadata": {
        "id": "Lni2bOTaovFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media_recompensa_treinamento_V4 = np.mean(resultado_medio_V4['Rewards_ep'])\n",
        "print(f\"A recompensa média do treinamento é: {media_recompensa_treinamento_V4}\")\n",
        "\n",
        "media_recompensa_avaliacao_V4 = np.mean(resultado_medio_V4['Rewards_ep_eval'])\n",
        "print(f\"A recompensa média da avaliação é: {media_recompensa_avaliacao_V4}\")\n",
        "\n",
        "media_taxa_sucesso_V4 = np.mean(resultado_medio_V4['Sucess_Rates'])\n",
        "print(f\"A taxa de sucesso média é: {media_taxa_sucesso_V4}\")"
      ],
      "metadata": {
        "id": "N1qhxHdwoxjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPhTFr2lhle4"
      },
      "source": [
        "# V4.1: PER Melhores Trajetórias"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descrição"
      ],
      "metadata": {
        "id": "VsweYm0qHnpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semelhante a PER Última Trajetória, mas nessa versão é criada uma lista com várias trajetória que atigiram a pontuação máxima."
      ],
      "metadata": {
        "id": "_fFmas3oHp4L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UApVuNzhlfA"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfbfGF0XhlfB"
      },
      "outputs": [],
      "source": [
        "reducao = \"none\"          # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V4.1\"            # Define o nome da experiência.\n",
        "etiqueta = \"video_V4.1\"               # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Agente V4.1"
      ],
      "metadata": {
        "id": "l_LkhdszhlfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V4_1:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_Amostra_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao            # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra, recompensa_maxima):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "        self.recompensa_maxima = recompensa_maxima        # Especifica a recompensa máxima que o agente pode atingir.\n",
        "\n",
        "        ### CONTROLE DO TREINAMENTO ###\n",
        "        Num_do_Teste               = 9\n",
        "        Quantidade_de_Treinamentos = 2\n",
        "        Criterio_de_Parada         = 5\n",
        "        Parada                     = False\n",
        "        Avaliacao_no_Treinamento   = False\n",
        "        Log_Estatico_dos_Result    = True\n",
        "\n",
        "        ### VARIÁVEIS UTILIZADAS ###\n",
        "        parada_500                = 0\n",
        "        indice_final              = 0\n",
        "        mean_recomp_ep            = []\n",
        "        # indices_incrementar     = []\n",
        "        parar                    = False\n",
        "        Valor_Anterior          = None\n",
        "        indices_incrementar     = deque(maxlen = 10_000)\n",
        "        incremento_prioridade   = torch.full((500,), 10)\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "            indices = self.rb.extend(dado)      # Grava dados no buffer de repetição\n",
        "\n",
        "            for l, passo_atual in enumerate(dado[\"next\",\"step_count\"]):\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.total_episodios += 1         # Atualiza o contador total de episódios completados.\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "                if passo_atual == self.recompensa_maxima:\n",
        "                # ############## CONDIÇÕES DE PARADA ################\n",
        "                #     parada_500 += 1\n",
        "                #     if Parada:\n",
        "                #         if parada_500 >= Criterio_de_Parada: stop = not stop\n",
        "                # if Avaliacao_no_Treinamento:\n",
        "                #     recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao = avaliar(self.ambiente,\n",
        "                #                                                                                                 self.politica_deterministica,\n",
        "                #                                                                                                 max_steps = passos_maximo)\n",
        "                #     if media_recompensas_avaliacao >= (recompensa_maxima * 0.99): parar = not parar\n",
        "                ###################################################\n",
        "                    ############################ MUDANÇA IMPLEMENTADA #################################\n",
        "                    indice_final = indices[-1]\n",
        "                    indice_500   = torch.nonzero(dado[\"next\",\"step_count\"] == 499, as_tuple=True)\n",
        "                    indices_Incremento = range(indice_final.item()+1 - 499 + indice_500[0].item()-499,\n",
        "                                         indice_final.item()+1 - 499 + indice_500[0].item()+1)\n",
        "                    for i in indices_Incremento:\n",
        "                        indices_incrementar.append(i)\n",
        "                    incremento_prioridade = torch.full((len(indices_incrementar),), 10)\n",
        "                    self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)\n",
        "                ###################################################################################\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if ((self.total_episodios >= self.episodios) and (Parada == False)): break\n",
        "            elif parar: break\n",
        "\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay.\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info = True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      # self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)                  # 2ª VEZ\n",
        "                      valores_erro[\"loss\"] = valores_erro[\"loss\"].mean()      # Usar PER Normal pois reduction = \"None\"\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                          # Atualiza o contador total de passos coletados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "            # if (Log_Estatico_dos_Result): clear_output(wait=True)\n",
        "            # Num_Treinamentos += 1\n",
        "            # Num_do_Teste += 1\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "beJX5Lr5hlfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "Ix-YckEYhlfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V4_1 = Mod_Agente_V4_1(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                  passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau,\n",
        "                                  caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V4_1.treinar_agente(episodios, tamanho_amostra, recompensa_maxima)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V4_1.ambiente, agente_V4_1.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V4.1\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "1Xbg5O8khlfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQNwNyc-hlfD"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3JNzGSVhlfD"
      },
      "outputs": [],
      "source": [
        "resultado_1_V4_1 = carrega_array(\"C1\",\n",
        "                  \"V4.1\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V4_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBo3x5_7hlfD"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V4.1\",\n",
        "                           inicio = 1,\n",
        "                           fim = 10,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV7_7oFChlfE"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V4_1 = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2IuYQ14hlfE"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4_1['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a5wlB2SnhlfE"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4_1['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "luxsHUInhlfE"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4_1['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DclaYfGPhlfF"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4_1['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Mwf0ShvohlfF"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4_1['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy4Q_eUMs_e8"
      },
      "source": [
        "# V4.3: PER Piores Trajetórias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP5gFk_ls_fI"
      },
      "source": [
        "## Parâmetros da Versão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUKKmJATs_fI"
      },
      "outputs": [],
      "source": [
        "reducao = \"none\"          # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "nome_experiencia = \"dqn_V4.3\"            # Define o nome da experiência.\n",
        "etiqueta = \"video_V4.3\"               # Uma tag para identificar os vídeos nos logs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Agente V4.3"
      ],
      "metadata": {
        "id": "vFKT6cMms_fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mod_Agente_V4_3:\n",
        "    def __init__(self, nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini, passos_otimizacao,\n",
        "                 tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau, caminho, nome_experiencia, formato_video, etiqueta):\n",
        "        # Estrutura do Agente\n",
        "        self.nome_jogo = nome_jogo\n",
        "        self.semente = semente\n",
        "        self.ambiente = criar_ambiente(self.nome_jogo, self.semente)\n",
        "        self.num_celulas = num_celulas\n",
        "        self.rede_valor = criar_rede_neural(self.ambiente, self.num_celulas)\n",
        "        self.passos_decaimento = passos_decaimento\n",
        "        self.epsilon_ini = epsilon_ini\n",
        "        self.epsilon_fim = epsilon_fim\n",
        "        self.politica_deterministica, self.modulo_exploracao, self.politica_exploratoria = criar_politica(self.ambiente, self.rede_valor, self.passos_decaimento, self.epsilon_ini, self.epsilon_fim)\n",
        "        self.exp_coletadas = exp_coletadas\n",
        "        self.passos_aleatorios_ini = passos_aleatorios_ini\n",
        "        self.passos_otimizacao = passos_otimizacao\n",
        "        self.coletor = criar_coletor(self.ambiente, self.politica_exploratoria, self.exp_coletadas, self.passos_aleatorios_ini)\n",
        "        self.tamanho_buffer = tamanho_buffer\n",
        "        self.alfa = alfa\n",
        "        self.beta = beta\n",
        "        self.rb = criar_Buffer_Replay_Amostra_PER(self.tamanho_buffer,  self.alfa, self.beta)\n",
        "        self.funcao_perda = funcao_perda\n",
        "        self.reducao = reducao            # Especifica a redução a ser aplicada à saída: \"none\"| \"mean\"| \"sum\"\n",
        "        self.perda = criar_perda(self.politica_deterministica, self.ambiente, self.funcao_perda, self.reducao)\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.otimizador = criar_otimizador(self.perda, self.taxa_aprendizado)\n",
        "        self.tau = tau\n",
        "        self.atualizador_rede_alvo = criar_atualizador_rede_alvo(self.perda, self.tau)\n",
        "\n",
        "        # Exibição por Vídeo\n",
        "        self.caminho = caminho                    # Define o diretório onde os logs e outros dados do treinamento serão salvos. Neste caso, é um diretório local chamado training_loop.\n",
        "        self.nome_experiencia = nome_experiencia  # Define o nome da experiência.\n",
        "        self.formato_video = formato_video        # Especifica o formato de vídeo para gravação.\n",
        "        self.etiqueta = etiqueta                  # Uma tag para identificar os vídeos nos logs.\n",
        "        self.registrador, self.gravador_video, self.ambiente_gravacao = criar_registrador(self.nome_experiencia, self.caminho, self.formato_video, self.etiqueta, self.nome_jogo)\n",
        "\n",
        "    def treinar_agente(self, episodios, tamanho_amostra, recompensa_maxima):\n",
        "        # Váriaveis para o treinamento\n",
        "        self.total_passos = 0                             # Inicializa um contador para o número total de passos coletados.\n",
        "        self.total_episodios = 0                          # Inicializa um contador para o número total de episódios completados.\n",
        "        self.passo_anterior = None                        # Inicializa uma variável para armazenar o último passo coletado.\n",
        "        self.tempo_ini = time.time()                      # Armazena o tempo de início do treinamento para calcular a duração total no final.\n",
        "        self.recompensas_treinamento = []                 # Inicializa uma lista para armazenar as recompensas de cada episódio.\n",
        "        self.recompensas_medias_treinamento = []          # Inicializa uma lista para armazenar as recompensas médias de cada 100 episódios.\n",
        "        self.episodios = episodios                        # Especifica o número total de episódios a serem treinados.\n",
        "        self.tamanho_amostra = tamanho_amostra            # Especifica o tamanho da amostra a ser coletada em cada iteração\n",
        "        self.recompensa_maxima = recompensa_maxima        # Especifica a recompensa máxima que o agente pode atingir.\n",
        "\n",
        "        ### CONTROLE DO TREINAMENTO ###\n",
        "        Num_do_Teste               = 9\n",
        "        Quantidade_de_Treinamentos = 2\n",
        "        Criterio_de_Parada         = 5\n",
        "        Parada                     = False\n",
        "        Avaliacao_no_Treinamento   = False\n",
        "        Log_Estatico_dos_Result    = True\n",
        "\n",
        "        ### VARIÁVEIS UTILIZADAS ###\n",
        "        parada_500                = 0\n",
        "        indice_final              = 0\n",
        "        mean_recomp_ep            = []\n",
        "        # indices_incrementar     = []\n",
        "        parar                    = False\n",
        "        Valor_Anterior          = None\n",
        "        indices_incrementar     = deque(maxlen = 10_000)\n",
        "        incremento_prioridade   = torch.full((500,), 10)\n",
        "\n",
        "        ### Loop principal do treinamento.\n",
        "        for i, dado in enumerate(self.coletor):\n",
        "            indices = self.rb.extend(dado)      # Grava dados no buffer de repetição\n",
        "\n",
        "            for l, passo_atual in enumerate(dado[\"next\",\"step_count\"]):\n",
        "                if self.passo_anterior is not None:\n",
        "                    if passo_atual < self.passo_anterior:\n",
        "                        self.total_episodios += 1         # Atualiza o contador total de episódios completados.\n",
        "                        self.recompensas_treinamento.append(self.passo_anterior.item())\n",
        "                        self.recompensas_medias_treinamento.append(np.mean(self.recompensas_treinamento[-100:]))\n",
        "                self.passo_anterior = passo_atual\n",
        "\n",
        "                if passo_atual == self.recompensa_maxima:\n",
        "                # ############## CONDIÇÕES DE PARADA ################\n",
        "                #     parada_500 += 1\n",
        "                #     if Parada:\n",
        "                #         if parada_500 >= Criterio_de_Parada: stop = not stop\n",
        "                # if Avaliacao_no_Treinamento:\n",
        "                #     recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao = avaliar(self.ambiente,\n",
        "                #                                                                                                 self.politica_deterministica,\n",
        "                #                                                                                                 max_steps = passos_maximo)\n",
        "                #     if media_recompensas_avaliacao >= (recompensa_maxima * 0.99): parar = not parar\n",
        "                ###################################################\n",
        "                ########################## MUDANÇA IMPLEMENTADA ############################\n",
        "                    base = torch.nonzero(dado[\"next\",\"step_count\"] < 100, as_tuple=True)[0]\n",
        "                    for i in base.tolist():\n",
        "                        indices_incrementar.append(indices[i].item())\n",
        "                    incremento_prioridade = torch.full((len(indices_incrementar),), 10)\n",
        "                    self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)\n",
        "                 ############################################################################\n",
        "\n",
        "            ### Condição de Parada\n",
        "            if ((self.total_episodios >= self.episodios) and (Parada == False)): break\n",
        "            elif parar: break\n",
        "\n",
        "            recorde_atual = self.rb[:][\"next\", \"step_count\"].max()   # Calcula o comprimento máx. dos episódios armazenados no buffer de replay.\n",
        "\n",
        "            ### Condição para Iniciar a Otimização\n",
        "            if len(self.rb) >= self.passos_aleatorios_ini:             # Verifica se o tamanho do buffer de replay é maior que init_rand_steps.\n",
        "                ### Loop de Otimização\n",
        "                  for _ in range(self.passos_otimizacao):              # Realiza várias etapas de otimização para cada lote de dados coletados para maior eficiência.\n",
        "                      if self.tamanho_amostra>len(self.rb):pass\n",
        "                      amostra, info = self.rb.sample(self.tamanho_amostra, return_info = True)   # Amostra 200 transições do buffer de replay.\n",
        "                      valores_erro = self.perda(amostra)               # Calcula os valores de perda usando a amostra\n",
        "                      self.rb.update_priority(index = info['index'], priority = valores_erro[\"loss\"])\n",
        "                      # self.rb.update_priority(index = indices_incrementar, priority = incremento_prioridade)                  # 2ª VEZ\n",
        "                      valores_erro[\"loss\"] = valores_erro[\"loss\"].mean()      # Usar PER Normal pois reduction = \"None\"\n",
        "                      valores_erro[\"loss\"].backward()                  # Calcula os gradientes da perda.\n",
        "                      self.otimizador.step()                           # Atualiza os parâmetros da rede usando o otimizador.\n",
        "                      self.otimizador.zero_grad()                      # Zera os gradientes acumulados para a próxima iteração.\n",
        "\n",
        "                      ### Atualização do Fator de Exploração\n",
        "                      self.modulo_exploracao.step(dado.numel())        # Atualiza o fator de exploração baseado no número de elementos em data.\n",
        "\n",
        "                      ### Atualização dos Parâmetros da Rede-alvo\n",
        "                      self.atualizador_rede_alvo.step()                # Realiza a atualização suave dos parâmetros da rede-alvo.\n",
        "\n",
        "            ### Registra informações sobre o número máximo de passos e o tamanho do buffer de replay.\n",
        "            self.total_passos += dado.numel()                          # Atualiza o contador total de passos coletados.\n",
        "\n",
        "            ### Monitoramento e Registro\n",
        "            if (self.total_episodios % 100)==0:\n",
        "                torchrl_logger.info(f\"Episódios: {self.total_episodios}, Recorde Atual: {recorde_atual}, Tamanho do Buffer {len(self.rb)}\")\n",
        "            # if (Log_Estatico_dos_Result): clear_output(wait=True)\n",
        "            # Num_Treinamentos += 1\n",
        "            # Num_do_Teste += 1\n",
        "        ### Cálculo e Registro do Tempo Total de Treinamento\n",
        "        tempo_fim = time.time()                               # Armazena o tempo de término do treinamento.\n",
        "        tempo_treinamento = tempo_fim - self.tempo_ini        # Calcula o tempo total de treinamento em segundos.\n",
        "        torchrl_logger.info(f\"Resolvido depois de {self.total_passos} passos, {self.total_episodios} episódios em {tempo_treinamento}s.\")\n",
        "\n",
        "        return self.recompensas_treinamento, self.recompensas_medias_treinamento, tempo_treinamento\n",
        "\n",
        "    def renderizar_video(self, maximo_passos = 1000 ):\n",
        "        # Método que realiza uma execução no ambiente, coletando observações, ações, recompensas e outras informações.\n",
        "        self.ambiente_gravacao.rollout(max_steps = maximo_passos, policy = self.politica_deterministica)\n",
        "\n",
        "        # Método que salva os vídeos gravados durante as execuções no ambiente para o disco, utilizando o logger configurado anteriormente.\n",
        "        self.gravador_video.dump()"
      ],
      "metadata": {
        "id": "iHpJK46ts_fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "ubUk16Ljs_fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulacao_inicial = 11\n",
        "simulacao_final = 11\n",
        "for semente in range(simulacao_inicial, (simulacao_final+1)):\n",
        "    # Cria o agente\n",
        "    agente_V4_3 = Mod_Agente_V4_3(nome_jogo, semente, num_celulas, passos_decaimento, epsilon_ini, epsilon_fim, exp_coletadas, passos_aleatorios_ini,\n",
        "                                  passos_otimizacao, tamanho_buffer, alfa, beta, funcao_perda, reducao, taxa_aprendizado, tau,\n",
        "                                  caminho, nome_experiencia, formato_video, etiqueta)\n",
        "\n",
        "    # Treina o agente\n",
        "    recompensas_treinamento, recompensas_medias_treinamento, tempo_treinamento = agente_V4_3.treinar_agente(episodios, tamanho_amostra, recompensa_maxima)\n",
        "\n",
        "    # Plota os resultado do Treinamento\n",
        "    plt.plot(recompensas_treinamento)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_treinamento)\n",
        "    plt.show()\n",
        "    print(f\"Tempo de Treinamento: {tempo_treinamento} segundos\")\n",
        "\n",
        "    # Avalia o agente\n",
        "    recompensas_avaliacao, recompensas_medias_avaliacao, media_recompensas_avaliacao, tempo_ini_avaliacao = avaliar(agente_V4_3.ambiente, agente_V4_3.politica_deterministica, max_steps = passos_maximo)\n",
        "\n",
        "    # Plota os resultados da Avaliação\n",
        "    plt.plot(recompensas_avaliacao)\n",
        "    plt.show()\n",
        "    plt.plot(recompensas_medias_avaliacao)\n",
        "    plt.show()\n",
        "    print(f\"Média das Recompensas da Avaliação: {media_recompensas_avaliacao}\")\n",
        "\n",
        "    # Calcula a Taxa de Sucesso da Avaliação e Plota o Resultado\n",
        "    Sucessos, Desempenho_Medio = taxa_sucesso(recompensas_medias_avaliacao)\n",
        "\n",
        "    # Calcula o Tempo de Avaliação e de Simulação\n",
        "    tempo_fim_avaliacao = time.time()\n",
        "    tempo_avaliacao = tempo_fim_avaliacao - tempo_ini_avaliacao\n",
        "    print(f\"Tempo de Avaliação: {tempo_avaliacao} segundos\")\n",
        "    tempo_simulacao = tempo_treinamento + tempo_avaliacao\n",
        "    print(f\"Tempo de Simulação: {tempo_simulacao} segundos\")\n",
        "\n",
        "    # Salva o resultado da simulação\n",
        "    caminho_destino = salvar_array(\n",
        "                  recompensas_treinamento,\n",
        "                  recompensas_medias_treinamento,\n",
        "                  recompensas_avaliacao,\n",
        "                  recompensas_medias_avaliacao,\n",
        "                  media_recompensas_avaliacao,\n",
        "                  Sucessos,\n",
        "                  Desempenho_Medio,\n",
        "                  tempo_treinamento,\n",
        "                  tempo_avaliacao,\n",
        "                  tempo_simulacao,\n",
        "                  cenario = \"C1\",\n",
        "                  versao = \"V4.3\",\n",
        "                  numero_simulacao = semente,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "xVpLoMxbs_fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMP10q1vs_fJ"
      },
      "source": [
        "## Carrega e Exibe Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8Sf15RYs_fJ"
      },
      "outputs": [],
      "source": [
        "resultado_1_V4_3 = carrega_array(\"C1\",\n",
        "                  \"V4.3\",\n",
        "                  numero_simulacao = 1,\n",
        "                  caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                  pasta = nome_pasta)\n",
        "resultado_1_V4_3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPllkJkbs_fK"
      },
      "outputs": [],
      "source": [
        "resultado = carrega_varios(\"C1\",\n",
        "                           \"V4.3\",\n",
        "                           inicio = 1,\n",
        "                           fim = 10,\n",
        "                           caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                           pasta = nome_pasta)\n",
        "resultado[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op_eB3TVs_fK"
      },
      "outputs": [],
      "source": [
        "resultado_medio_V4_3 = resultado_medio(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVeR9vVss_fK"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4_3['Rewards_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8Ma_Tz-Os_fK"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4_3['Rewards_mean_ep'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = episodios,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2Kd87ahMs_fK"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4_3['Rewards_ep_eval'],\n",
        "       xlabel = 'Episódios',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 1000,\n",
        "       ylimit = 500,\n",
        "       title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iDHVueHzs_fL"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4_3['Rewards_mean_ep_eval'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Recompensa Total',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 500,\n",
        "       title = 'Desempenho Médio por Episódio nas Avaliações')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "77WIiIbIs_fL"
      },
      "outputs": [],
      "source": [
        "plotar(resultado_medio_V4_3['Sucess_Rates'],\n",
        "       xlabel = 'Testes',\n",
        "       ylabel = 'Taxa de Sucesso (%)',\n",
        "       limit = True,\n",
        "       xlimit = 10,\n",
        "       ylimit = 100,\n",
        "       title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparativo de Resultados"
      ],
      "metadata": {
        "id": "ChX6Rw6UdFNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultado_medio_V1, resultado_medio_V2_rb, resultado_medio_V2_1, resultado_medio_V2_2, resultado_medio_V3_1v, resultado_medio_V4 = carrega_media_versoes(cenario = \"C1\",\n",
        "                          # versao = [\"V1\", \"V2_rb\", \"V2_prb\", \"V2.1\", \"V2.2\", \"V3_1v\", \"V3_2v\", \"V3_1\", \"V3.2_sm1\", \"V3.2_sm2\", \"V3_3\", \"V3.4_sm1\", \"V3.4_sm2\", \"V4\", \"V4.1\", \"V4.2\", \"V4.3\" ],\n",
        "                          # versao = [\"V1\", \"V2_rb\", \"V2.1\", \"V2.2\", \"V3_1v\", \"V3_1\", \"V3.2_sm1\", \"V3_3\", \"V3.4_sm2\", \"V4\", \"V4.1\", \"V4.3\" ],\n",
        "                          versao = [\"V1\", \"V2_rb\", \"V2.1\", \"V2.2\", \"V3_1v\", \"V4\" ],\n",
        "                          inicio = 1,\n",
        "                          fim = 20,\n",
        "                          caminho = '/content/drive/My Drive/CBA 2024/Resultados/',\n",
        "                          pasta = nome_pasta)"
      ],
      "metadata": {
        "id": "PAhzi-bpdJv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotar_versoes(    coluna = 'Rewards_ep',\n",
        "                   xlabel = 'Episódios',\n",
        "                   ylabel = 'Recompensa Total',\n",
        "                   tamanho_fig = (12, 8),\n",
        "                   grade = False,\n",
        "                   limit = False,\n",
        "                   xlimit = episodios,\n",
        "                   ylimit = 500,\n",
        "                   title = 'Média dos Desempenhos por Episódio nos Treinamentos')"
      ],
      "metadata": {
        "id": "8wzIfu44gFt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotar_versoes(    coluna = 'Rewards_mean_ep',\n",
        "                   xlabel = 'Episódios',\n",
        "                   ylabel = 'Recompensa Total',\n",
        "                   tamanho_fig = (12, 8),\n",
        "                   grade = False,\n",
        "                   limit = False,\n",
        "                   xlimit = episodios,\n",
        "                   ylimit = 500,\n",
        "                   title = 'Desempenho Médio por Episódio nos Treinamentos')"
      ],
      "metadata": {
        "id": "LAPd_qcegY9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotar_versoes(    coluna = 'Rewards_ep_eval',\n",
        "                   xlabel = 'Episódios',\n",
        "                   ylabel = 'Recompensa Total',\n",
        "                   tamanho_fig = (12, 8),\n",
        "                   grade = False,\n",
        "                   limit = False,\n",
        "                   xlimit = 1000,\n",
        "                   ylimit = 500,\n",
        "                   title = 'Média dos Desempenhos por Episódio nas Avaliações')"
      ],
      "metadata": {
        "id": "MfBLwGG0gcbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotar_versoes(    coluna = 'Rewards_mean_ep_eval',\n",
        "                   xlabel = 'Testes',\n",
        "                   ylabel = 'Recompensa Total',\n",
        "                   tamanho_fig = (12, 8),\n",
        "                   grade = False,\n",
        "                   limit = False,\n",
        "                   xlimit = 9,\n",
        "                   ylimit = 500,\n",
        "                   title = 'Desempenho Médio por Episódio nas Avaliações')"
      ],
      "metadata": {
        "id": "PSsDpKirgfAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotar_versoes(    coluna = 'Sucess_Rates',\n",
        "                   xlabel = 'Testes',\n",
        "                   ylabel = 'Taxa de Sucesso (%)',\n",
        "                   tamanho_fig = (12, 8),\n",
        "                   grade = False,\n",
        "                   limit = False,\n",
        "                   xlimit = 10,\n",
        "                   ylimit = 100,\n",
        "                   title = 'Média das Taxas de Sucesso  nas Avaliações')"
      ],
      "metadata": {
        "id": "fgejjQIRghvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uu-WIYcX9mHM",
        "WXpB_90z9Wi7",
        "2BJ6-pb8-hoi",
        "2UmYgAQSkLdv",
        "C4F4pUkqFNBr",
        "8wH2IHjfFNBs",
        "MkjElFfR6M8J",
        "D-rheTgo6Pl4",
        "wW4J0O7Nudi2",
        "NvA3FYLaOWnR",
        "Gebr-b5IPPWM",
        "9kK9SfbpPTkq",
        "O8EGGc2ZPcBR",
        "ZRrANVhGvyIH",
        "ZeGeXQHsdIry",
        "ZuL6rQ7mFOPq",
        "WuHn9QXf7OyQ",
        "bLLhpYzv7ga1",
        "v5Xj4AbDCTok",
        "aNcegwE_1UyB",
        "L1cmgjkulNNu",
        "zl-xqbYxX5-G",
        "hFTanQvMUZ_X",
        "Xk2F3wTRczfC",
        "2XGks7Fg-V3r",
        "E4voKNlf31S2",
        "13qYpvBD2oZh",
        "eg8hJFJ4lHrP",
        "iV0jQ22aofx1",
        "D9lcka8E-V4K",
        "Y9EeR8i_qrAD",
        "sFc9rNS44EhC",
        "0kimGwSNqmzP",
        "9B8AqHaj3EYD",
        "LL5EopQjqrAf",
        "fOT3XF37NKjj",
        "q8QaiphINKjv",
        "pbrcwVuQEE00",
        "iCavwONiEaf6",
        "ODHBnEPFNKj6",
        "7Vk9VV-1mVXt",
        "ohkj6RMCmVX6",
        "mcRJ-vdrl9zp",
        "LWocUGIOn32o",
        "bFoXxQemmVYJ",
        "FhM98GsKJLSm",
        "u94Ym_3sJLSx",
        "USUOW5nkJLSy",
        "cEjlYQTOJLSz",
        "y42T5lnnJLS2",
        "MiCqB0ejAkzG",
        "zi8u1VtNJLS2",
        "tfl__DCzJLS3",
        "cfVqvIaCJLS4",
        "bvQNXJZjJLS4",
        "h2qZ5hneXNf7",
        "OnztUYPKXNgL",
        "ja0KjdT9kS0T",
        "M96aa3WHwT6K",
        "OTNa9Qh9XNgQ",
        "0j7nn3Nw1I1j",
        "R5gVmauJ1I1v",
        "Ry-Qe9SfDjGT",
        "Arg4AIoU1I1z",
        "RobDUuLaZrg8",
        "OMLdhNj9ZrhN",
        "gUyVqm8rZrhO",
        "tGPIKtvtZrhO",
        "IPhTFr2lhle4",
        "2UApVuNzhlfA",
        "l_LkhdszhlfB",
        "Ix-YckEYhlfC",
        "vQNwNyc-hlfD",
        "Wy4Q_eUMs_e8",
        "ZP5gFk_ls_fI",
        "vFKT6cMms_fI",
        "ubUk16Ljs_fJ",
        "NMP10q1vs_fJ",
        "ChX6Rw6UdFNd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
