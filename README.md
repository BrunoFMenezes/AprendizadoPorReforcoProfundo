# Aprendizado Por Refor√ßo Profundo
## Ebook DQN usando TorchRL
[Clique aqui para ir para o diret√≥rio do Ebook üìö](https://github.com/BrunoFMenezes/prompts-recipe-to-create-a-ebook/tree/main)
## Classes de ReplayBuffer da TorchRL
https://github.com/pytorch/rl/blob/main/torchrl/data/replay_buffers/replay_buffers.py#L901
## Classes de Samplers da TorchRL
https://github.com/pytorch/rl/blob/main/torchrl/data/replay_buffers/samplers.py
### Tutorial Prioritazy
https://pytorch.org/rl/stable/reference/generated/torchrl.data.replay_buffers.PrioritizedSampler.html?highlight=prioritizedsampler#torchrl.data.replay_buffers.PrioritizedSampler
## Melhorias para as Simula√ß√µes
1. Adotar a rela√ß√£o 80-20. Pontua√ß√£o acima de 80% √© boa e abaixo de 20% √© ruim.
## Ideias de Novas Vers√µes
1. PER Buffer Duplo: come√ßa usando um buffer e gera outro buffer com as melhores/piores trajet√≥rias(experi√™ncias), ap√≥s 50% do treinamento migra para o outro buffer e continua armanzenando nos dois buffer, um com as trajet√≥rias normais e outro s√≥ com as melhores/piores trajet√≥rias.
2. [ COMPLETE A LISTA]
